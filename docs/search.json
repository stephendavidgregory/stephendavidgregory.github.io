[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a conservation biologist and I use population modelling to better understand and propose management actions for a variety of animal populations, from native Galapagos rodents to Malaysian orangutans.\nCurrently, my research focuses on the effects of environmental drivers of salmonid vital rates and consequences for their population dynamics."
  },
  {
    "objectID": "about.html#my-skills",
    "href": "about.html#my-skills",
    "title": "About",
    "section": "My skills",
    "text": "My skills"
  },
  {
    "objectID": "about.html#my-professional-memberships",
    "href": "about.html#my-professional-memberships",
    "title": "About",
    "section": "My professional memberships",
    "text": "My professional memberships\n\nMember of the ICES Working Group on North Atlantic Salmon (2019-ongoing)\nMember of the ICES Workshop for North Atlantic Salmon At-Sea Mortality (2019-ongoing)\nMember of the National Centre for Statistical Ecology (2017-ongoing)\nReview Editor for Endangered Species Research"
  },
  {
    "objectID": "about.html#my-profile-elsewhere",
    "href": "about.html#my-profile-elsewhere",
    "title": "About",
    "section": "My profile elsewhere",
    "text": "My profile elsewhere\n ORCID: 0000-0002-8230-0191\n ResearchGate: Stephen_Gregory2\n Google Scholar: UAYCYvQAAAAJ\n Open Science Foundation: wtacd"
  },
  {
    "objectID": "about.html#my-cv",
    "href": "about.html#my-cv",
    "title": "About",
    "section": "My CV",
    "text": "My CV"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCJS is fast in TMB\n\n\nA simple Cormack Jolly Seber model is fast in Template Model Builder\n\n\n\n\nstatistics\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nMeet Tweedie\n\n\nAn exploration of the Tweedie family of distributions\n\n\n\n\nstatistics\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding co-occurrence\n\n\nA randomisation approach\n\n\n\n\nstatistics\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nNew paper: Environmental conditions modify density‐dependent salmonid recruitment: Insights into the 2016 recruitment crash in Wales\n\n\nPaper published in Freshwater Biology\n\n\n\n\nstatistics\n\n\ncode\n\n\nenvironmental-change\n\n\n\n\n\\(^{0}\\) Formerly Salmon & Trout Research Centre, Game and Wildlife Conservation Trust, East Stoke, Wareham, Dorset BH20 6BB, UK \\(^{1}\\) The Centre for Environment…\n\n\n\n\n\n\nSep 4, 2020\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\n(Mixed) Logistic regression in JAGS\n\n\nFitting fixed and contrasting specifications of mixed effects logistic regression in jags using dclone\n\n\n\n\nenvironmental-change\n\n\ntagging-and-telemetry\n\n\nallee-effects\n\n\ninvasive-species\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nNew paper: Scale-dependent drivers of Atlantic salmon parr lengths\n\n\nPaper published in Freshwater Biology\n\n\n\n\nenvironmental-change\n\n\ntagging-and-telemetry\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nRunning JAGS in parallel\n\n\nExamples using dclone, foreach, snow and snowfall\n\n\n\n\nenvironmental-change\n\n\ntagging-and-telemetry\n\n\nallee-effects\n\n\ninvasive-species\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nGet ready… set… go! Frome salmon smolt run 2017\n\n\nQuantifying the number of migrating juvenile Atlantic salmon smolts leaving the river Frome, Dorset\n\n\n\n\nenvironmental-change\n\n\ntagging-and-telemetry\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nOrdered factors and their analysis\n\n\nUnderstanding an ordered factor and implications for their use in statistical analysis\n\n\n\n\nstatistics\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nCalculate distances along a river in R and GRASS\n\n\nLeast cost distances between points on a river\n\n\n\n\nenvironmental-change\n\n\ntagging-and-telemetry\n\n\ninvasive-species\n\n\nallee-effects\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nNew paper: Flow and Atlantic salmon redd distribution\n\n\nSalmon redds clustered in mid-river when flow is low, with consequences for density-dependent mortality in emerging fry.\n\n\n\n\nenvironmental-change\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nPatterns on a parr: talk\n\n\nGWCT Tuesday Research Seminar on environmental drivers of salmon parr length changes, January 2017\n\n\n\n\nenvironmental-change\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nCoordinate conversions in R\n\n\nBritish National Grid to latitude and longitude conversion\n\n\n\n\nenvironmental-change\n\n\ntagging-and-telemetry\n\n\nallee-effects\n\n\ninvasive-species\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2017\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nQuadratic interaction terms fitted by Bayesian Variable Selection\n\n\nFitting quadratic interactions in jags under Kuo & Mallik variable selection\n\n\n\n\nstatistics\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nTamar visit\n\n\nVisiting the river Tamar, Devon/Cornwall, with the Environment Agency\n\n\n\n\ntagging-and-telemetry\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nGrayling Society 40th Annual Symmposium: talk\n\n\nA talk given at the Grayling Society 40th Annual Symposium\n\n\n\n\nenvironmental-change\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nEstimating sea trout smolt migration risks: update\n\n\nA chapter on our River Frome work in the 2nd International Sea Trout Symposium proceedings\n\n\n\n\ntagging-and-telemetry\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nEstimating sea trout smolt migration risks: talk\n\n\nTalk to the Second IFM Tagging and Telemetry Workshop, Edinburgh 2016\n\n\n\n\ntagging-and-telemetry\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nRedds in space\n\n\nA study by Elinor Parry on the spawning migration of salmon returning to the River Frome\n\n\n\n\nenvironmental-change\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nSalmonids in the floods\n\n\nHow do salmonids cope in the floods: discuss\n\n\n\n\nenvironmental-change\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\n  \n\n\n\n\nEstimating sea trout smolt migration risks\n\n\nAnalysis code for a simple state-space model to estimate acoustically tagged sea trout migration risks\n\n\n\n\ntagging-and-telemetry\n\n\nstatistics\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2016\n\n\nStephen D. Gregory\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "stephendavidgregory@gmail.com\n +44 (0) 1305 206703\n Dorset, UK\n 08h00 - 16h00 on Mon, Weds Thurs & 08h00 - 14h30 on Tues and Fri\n stephendavidgregory\n stephendgregory"
  },
  {
    "objectID": "contact.html#my-profile-elsewhere",
    "href": "contact.html#my-profile-elsewhere",
    "title": "Contact",
    "section": "My profile elsewhere",
    "text": "My profile elsewhere\n ORCID: 0000-0002-8230-0191\n ResearchGate: Stephen_Gregory2\n Google Scholar: UAYCYvQAAAAJ\n Open Science Foundation: wtacd"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "   Stephen D. Gregory",
    "section": "",
    "text": "Welcome\nI’m an ecologist, conservationist, fisheries scientists, and statistician. I have been fortunate to work in many wonderful places, with many wonderful people. In this blog I hope to share some of the knowledge that they nurtured in me…\nYou can find me on Mastodon or GitHub. Feel free to email me.\n\n\n\n\n\n\nCredit: the great xkcd\n\n\n\n\nLatest blogs\n\n\n\n\n\n\n\n\n\n\nCJS is fast in TMB\n\n\n\n\n\n\nStephen D. Gregory\n\n\nMar 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeet Tweedie\n\n\n\n\n\n\nStephen D. Gregory\n\n\nMar 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding co-occurrence\n\n\n\n\n\n\nStephen D. Gregory\n\n\nFeb 9, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nResearch Themes\nMy research themes include:\n\nEnvironmental change\nTagging and telemetry\nAllee effects\nInvasive species\n\n\n\nStatistics & code\nStatistics underpin my research and can be found at:\n\nGithub\nBlogs with statistics\nBlogs with code\nAnd in supplementary materials of my publications"
  },
  {
    "objectID": "posts/elinor/index.html",
    "href": "posts/elinor/index.html",
    "title": "Redds in space",
    "section": "",
    "text": "Elinor Parry did her MSc thesis with the GWCT Fisheries Team from February to August 2015 on the effect of flow on the distribution of salmon redds in the Frome.\n\n\n\nMap of the river Frome, Dorset, UK, showing the redd survey sections analysed.\n\n\nElinor found that flow effects salmon redd distribution in ways that she predicted: redds were more aggregated in the mid-river in years of low flow compared to years of high flow. This suggests that spawning salmon have difficultly accessing spawning grounds higher in the Frome catchment when there is insufficient water in the river between October and January.\n\n\n\nDiagram showing how the density of redds is predicted to change with distance from the tidal limit under low, medium and high flow conditions.\n\n\nThere are important management implications from Elinor’s study, particularly in light of the forecast increase in frequency and intensity of extreme floods and droughts under climate change and forecast increases in the Dorset human population size. Her findings suggest that we should manage water levels sensitively, particularly between October and January, to ensure that salmon can distribute their redds throughout all of the available spawning habitat, which will give the emerging fry the best chance of survival.\nElinor’s study was very challenging. Among the many difficulties she had to overcome, three were particularly prominent:\n\nShe had the arduous task of collating incomplete and inconsistent datasets into a single manageable database that she could use to test her predictions;\nShe had the near impossible task of deciding how to summarise river discharge data in a way relevant to salmon spawning migrations on the Frome; and\nShe had to devise a statistical analysis to describe salmon redd distributions using only a few simple explanatory variables.\n\nIn completing these tasks, Elinor demonstrated great care, attention to detail, innovative thinking and an ability to critically appraise literature and information.\nHer MSc project will soon be published in the Ecology of Freshwater Fish.\nShe submitted her thesis to the Cardiff University School of Biosciences in September and was awarded a well-earned Distinction – congratulations Elinor!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Redds in Space},\n  date = {2016-09-01},\n  url = {https://stephendavidgregory.github.io/posts/elinor},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Redds in Space.” September 1,\n2016. https://stephendavidgregory.github.io/posts/elinor."
  },
  {
    "objectID": "posts/grayling-society-symposium/index.html",
    "href": "posts/grayling-society-symposium/index.html",
    "title": "Grayling Society 40th Annual Symmposium: talk",
    "section": "",
    "text": "I recently attended the Grayling Society’s 40th Annual Symposium, held in Winchester in October 2016. I was invited to talk about the Wylye Grayling Study (p. 14-15 of the GWCT Annual Fisheries report). (I acknowledge the continuing support of Natural Resources Wales, the Piscatorial Society and the Grayling Research Trust.)\nMy slides are published here: Grayling_Society_Annual_Symposium_2016.pdf.\nThe meeting was excellent. There were a few notable talks: Dr Mark Everard (chaired by Steve Skuce) gave an very informative talk about grayling and their habitat requirements; Paul Knight Salmon and Trout Conservation UK (chaired by Alex Adams) gave an excellent and highly relevant talk about the state of UK chalkstreams; and Vanessa Huml (chaired by Bob Male) presented preliminary results from her PhD study, which were very promising. My talk (chaired by Robin Mulholland) was very well attended (it was the Grayling Society’s best ever attended annual symposium) and I received many questions and compliments.\nA huge thanks to the Grayling Society for the invitation to talk at their annual symposium. I look forward to seeing you again next year!\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Grayling {Society} 40th {Annual} {Symmposium:} Talk},\n  date = {2016-11-14},\n  url = {https://stephendavidgregory.github.io/posts/grayling-society-symposium},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Grayling Society 40th Annual\nSymmposium: Talk.” November 14, 2016. https://stephendavidgregory.github.io/posts/grayling-society-symposium."
  },
  {
    "objectID": "posts/holbrook/index.html",
    "href": "posts/holbrook/index.html",
    "title": "Estimating sea trout smolt migration risks",
    "section": "",
    "text": "I’m involved with a project aiming to understand brown trout (Salmo trutta L.) movements in freshwater and estuarine environments.\nHere, I want to share the methods we used to estimate the risks to brown trout when they migrate as sea trout smolts.\nBut first a little background.\n\nBackground\nBrown trout are funny fish.\nSome spend their entire lives in freshwater, e.g., a river, and are known as brown trout. Others migrate to marine waters, e.g., an estuary or sea, and are known as sea trout.\nSea trout return to freshwater a lot larger than brown trout, after feasting on the abundant food available in marine waters, and are able to produce more eggs. Brown trout might be smaller but they don’t run the risk of being eaten by any of the abundant marine predators.\nClearly, there is a fitness trade-off between getting big & producing more eggs (maximise your potential reproduction) and avoiding being eaten by any one of the abundant marine predators (maximise your potential survival to reproduction).\nThe decision to migrate is not, however, black and white. Finnock are brown trout that use estuaries and transitional waters for only short periods, e.g., a couple of months, presumably to feed.\nSo which strategy is best: brown trout, finnock or sea trout?\nTo study this question, we estimated the risk to sea trout of migrating through fresh, transitional and estuarine waters.\nTo estimate these risks, we acoustically tagged a sample of migrating sea trout smolts who’s migration pathways were recorded at listening receivers located in the different zones (Figure 1).\n\n\n\nLocation of listening receivers in Poole Harbour. Red dots represent approximate detection range of the receivers.\n\n\n\n\nAnalysis methods\nAcoustic tracking data has the problem that detection is imperfect; we do not always detect a passing tag and so we don’t know if (i) the tag didn’t pass or (ii) whether it passed but was not detected.\nThis problem can and should be addressed statistically.\nTo estimate the risks to sea trout smolt of migrating through different zones, we used Bayesian State Space models (wikipedia: State-space representation).\nI was reassured that others have used BSSMs in this context: Oliviez Gimenez’s paper explains clearly the theory of BSSMs for marked individuals and Chris Holbrook’s paper was an illustrative example of BSSM implementation for acoustically tagged lamprey.\nIn essence, BSSM estimates jointly the probability that a tag is detected at a particular location and the probability that it made the transition to that location successfully.\nIn our study, we assumed that all individuals shared the same detection and transition probabilities, i.e., that physical or behavioural differences between individuals were unimportant, and that individuals travelled independently.\nWe could therefore use the simple Cormack, Jolly & Seber (CJS) model given by:\n\\[\nY_t|X_t \\sim Binomial(X_t − u_t, p_t)\n\\]\n\\[\nX_{t+1}|X_t \\sim Binomial(X_t, \\phi_t) + u_{t+1}\n\\]\nwhere \\(X_t\\) is the total number of survivors from time \\(t\\), which includes \\(u_t\\) that is the number of newly marked individuals at time \\(t\\), \\(Y_t\\) is the total number of previously marked individuals encountered at time \\(t\\), \\(p_t\\) is the probability of detecting a tagged individual at time \\(t\\) (\\(t = 2, ..., T\\)) and \\(\\phi_t\\) is the probability that a tagged individual transitions to time \\(t + 1\\) given that it is alive at time \\(t\\) (\\(t = 1, ..., T − 1\\)).\nThis formulation separates the nuisance parameters (the detection probabilities, \\(p_t\\)) from the parameters of interest (the transition probabilities, \\(\\phi_t\\)) because the latter are found only in the second or “state” equation.\nUsing this model, we estimated values of \\(p_t\\) and \\(\\phi_t\\) using the Monte Carlo Markov Chain (MCMC) method in JAGS. JAGS uses Gibbs sampling to explore the joint probability distribution of \\(p_t\\) and \\(\\phi_t\\). Through an iterative process, weakly informative \\(Beta(1, 1)\\) prior distributions on \\(p_t\\) and \\(\\phi_t\\) were updated with increasingly credible values until, after sufficient iterations, the best estimated values of \\(p_t\\) and \\(\\phi_t\\) were taken to be the median of their posterior distributions.\nWe ran JAGS from within R using functions from package dclone. We ran three MCMC chains for 30,000 iterations, of which we discarded the first 10,000 as burnin.\nR code to run an example BSSM is as follows:\nThe JAGS model file for the example might be BSSM.jags:\n\n# simulate data\nreceivor_efficiencies &lt;- c('r1' = 0.8, 'r2' = 0.9, 'r3' = 0.6, 'r4' = 0.7, 'r5' = 0.7, 'r6' = 0.7)\nn_fish &lt;- 77\nch_m &lt;- matrix(NA, ncol = length(receivor_efficiencies), nrow = n_fish)\nfor(i in 1:length(receivor_efficiencies)){\n    ch_m[, i] &lt;- sample(c(0, 1), n_fish, TRUE, c((1 - receivor_efficiencies[i]), receivor_efficiencies[i]))\n}\nch_m &lt;- data.frame('release' = 1, ch_m)\ncolnames(ch_m)[-1] &lt;- names(receivor_efficiencies)\n\n# create state and observation matrices\nsm &lt;- ch_m\nsm[sm == '0'] &lt;- NA\nom &lt;- ch_m\n\n# prep data\nd &lt;- list('sm' = sm,\n          'om' = om,\n          'N' = n_fish,\n          'T' = ncol(ch_m))\n\n# load libraries\nlibrary(dclone)\n\nLoading required package: coda\n\n\nLoading required package: parallel\n\n\nLoading required package: Matrix\n\n\ndclone 2.3-0     2019-03-21\n\nlibrary(rjags)\n\nLinked to JAGS 4.3.1\n\n\nLoaded modules: basemod,bugs\n\n# name of model file\nmf &lt;- function() {\n\n  # define likelihoods\n  for(i in 1:N){\n    for(t in 2:T){\n\n      # state model\n      sm[i, t] ~ dcat(phi[t - 1, sm[i, t - 1], ])\n\n      # observation model\n      om[i, t] ~ dbern(p[t, sm[i, t]])\n\n    }\n  }\n\n  # detection probability priors and constraints\n\n  # release\n  p[1, 1] &lt;- 1 # always observed at release\n  p[1, 2] &lt;- 0\n\n  # r1\n  p[2, 1] ~ dbeta(1, 1) # flat prior\n  p[2, 2] &lt;- 0\n\n  # r2\n  p[3, 1] ~ dbeta(1, 1) # flat prior\n  p[3, 2] &lt;- 0\n\n  # r3\n  p[4, 1] ~ dbeta(1, 1) # flat prior\n  p[4, 2] &lt;- 0\n\n  # r4\n  p[5, 1] ~ dbeta(1, 1) # flat prior\n  p[5, 2] &lt;- 0\n\n  # r5\n  p[6, 1] ~ dbeta(1, 1) # flat prior\n  p[6, 2] &lt;- 0\n\n  # r6\n  p[7, 1] ~ dbeta(1, 1) # flat prior\n  p[7, 2] &lt;- 0\n\n  # transition probability priors and constraints\n\n  # 1st transition\n  phi[1, 1, 1] ~ dbeta(1, 1)\n  phi[1, 1, 2] &lt;- 1 - phi[1, 1, 1]\n  phi[1, 2, 1] &lt;- 0\n  phi[1, 2, 2] &lt;- 1\n\n  # 2nd transition\n  phi[2, 1, 1] ~ dbeta(1, 1)\n  phi[2, 1, 2] &lt;- 1 - phi[2, 1, 1]\n  phi[2, 2, 1] &lt;- 0\n  phi[2, 2, 2] &lt;- 1\n\n  # 3rd transition\n  phi[3, 1, 1] ~ dbeta(1, 1)\n  phi[3, 1, 2] &lt;- 1 - phi[3, 1, 1]\n  phi[3, 2, 1] &lt;- 0\n  phi[3, 2, 2] &lt;- 1\n\n  # 4th transition\n  phi[4, 1, 1] ~ dbeta(1, 1)\n  phi[4, 1, 2] &lt;- 1 - phi[4, 1, 1]\n  phi[4, 2, 1] &lt;- 0\n  phi[4, 2, 2] &lt;- 1\n\n  # 5th transition\n  phi[5, 1, 1] ~ dbeta(1, 1)\n  phi[5, 1, 2] &lt;- 1 - phi[5, 1, 1]\n  phi[5, 2, 1] &lt;- 0\n  phi[5, 2, 2] &lt;- 1\n\n  # 6th transition\n  phi[6, 1, 1] ~ dbeta(1, 1)\n  phi[6, 1, 2] &lt;- 1 - phi[6, 1, 1]\n  phi[6, 2, 1] &lt;- 0\n  phi[6, 2, 2] &lt;- 1\n\n}\n\n# parameters to monitor\np &lt;- c('p', 'phi')\n\n# initialise model\nm &lt;- jagsModel(mf, data = d, n.chains = 1, n.adapt = 1000, quiet = FALSE)\n\nRegistered S3 method overwritten by 'R2WinBUGS':\n  method            from  \n  as.mcmc.list.bugs dclone\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 800\n   Unobserved stochastic nodes: 136\n   Total graph size: 1331\n\nInitializing model\n\nupdate(m, n.iter = 10000)\n\n# coda samples\ns &lt;- codaSamples(m, p, n.iter = 20000)\n\n# plot traces\nplot(s, trace = TRUE, density = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# plot densities\nplot(s, trace = FALSE, density = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# summary\ns.tab &lt;- summary(s)\nprint(s.tab$statistics, digits = 3)\n\n             Mean     SD Naive SE Time-series SE\np[1,1]     1.0000 0.0000 0.00e+00       0.000000\np[2,1]     0.8226 0.0424 3.00e-04       0.000295\np[3,1]     0.8989 0.0336 2.37e-04       0.000237\np[4,1]     0.5384 0.0571 4.04e-04       0.000404\np[5,1]     0.7078 0.0523 3.70e-04       0.000403\np[6,1]     0.8134 0.0488 3.45e-04       0.000550\np[7,1]     0.8130 0.1059 7.49e-04       0.003669\np[1,2]     0.0000 0.0000 0.00e+00       0.000000\np[2,2]     0.0000 0.0000 0.00e+00       0.000000\np[3,2]     0.0000 0.0000 0.00e+00       0.000000\np[4,2]     0.0000 0.0000 0.00e+00       0.000000\np[5,2]     0.0000 0.0000 0.00e+00       0.000000\np[6,2]     0.0000 0.0000 0.00e+00       0.000000\np[7,2]     0.0000 0.0000 0.00e+00       0.000000\nphi[1,1,1] 0.9872 0.0126 8.93e-05       0.000211\nphi[2,1,1] 0.9872 0.0125 8.87e-05       0.000211\nphi[3,1,1] 0.9749 0.0201 1.42e-04       0.000332\nphi[4,1,1] 0.9829 0.0162 1.14e-04       0.000273\nphi[5,1,1] 0.9525 0.0340 2.40e-04       0.000671\nphi[6,1,1] 0.8180 0.1051 7.44e-04       0.003848\nphi[1,2,1] 0.0000 0.0000 0.00e+00       0.000000\nphi[2,2,1] 0.0000 0.0000 0.00e+00       0.000000\nphi[3,2,1] 0.0000 0.0000 0.00e+00       0.000000\nphi[4,2,1] 0.0000 0.0000 0.00e+00       0.000000\nphi[5,2,1] 0.0000 0.0000 0.00e+00       0.000000\nphi[6,2,1] 0.0000 0.0000 0.00e+00       0.000000\nphi[1,1,2] 0.0128 0.0126 8.93e-05       0.000211\nphi[2,1,2] 0.0128 0.0125 8.87e-05       0.000211\nphi[3,1,2] 0.0251 0.0201 1.42e-04       0.000332\nphi[4,1,2] 0.0171 0.0162 1.14e-04       0.000273\nphi[5,1,2] 0.0475 0.0340 2.40e-04       0.000671\nphi[6,1,2] 0.1820 0.1051 7.44e-04       0.003848\nphi[1,2,2] 1.0000 0.0000 0.00e+00       0.000000\nphi[2,2,2] 1.0000 0.0000 0.00e+00       0.000000\nphi[3,2,2] 1.0000 0.0000 0.00e+00       0.000000\nphi[4,2,2] 1.0000 0.0000 0.00e+00       0.000000\nphi[5,2,2] 1.0000 0.0000 0.00e+00       0.000000\nphi[6,2,2] 1.0000 0.0000 0.00e+00       0.000000\n\n\nwhere d is a list of data passed to JAGS that includes:\n\n\\(N\\) = number of individuals\n\\(T\\) = number of occasions\n\\(sm[i, t]\\) = true state matrix\n\\(om[i, t]\\) = observation matrix\n\nwhere \\(i\\) is individual and \\(t\\) is occasion.\nOur model ran without problems and the results were intuitive and as expected. We feel that this procedure worked well for us.\nYou will be able to read about the results in a future post.\nIn the meantime, use the code above to run an example BSSM and contact me if you have any problems.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Estimating Sea Trout Smolt Migration Risks},\n  date = {2016-01-28},\n  url = {https://stephendavidgregory.github.io/posts/holbrook},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Estimating Sea Trout Smolt Migration\nRisks.” January 28, 2016. https://stephendavidgregory.github.io/posts/holbrook."
  },
  {
    "objectID": "posts/holbrookii/index.html",
    "href": "posts/holbrookii/index.html",
    "title": "Estimating sea trout smolt migration risks: update",
    "section": "",
    "text": "We wrote a chapter for the 2nd International Sea Trout Symposium proceedings.\n\nAbstract\nMigration between freshwater nursery grounds and saltwater feeding areas is a critical event in sea trout life history. During their seaward migration, smolts encounter both natural and man-made obstructions as well as increased exposure to predators. This is particularly true in the transition zone between freshwater and saltwater where smolts enter a new environment, change their behaviour and encounter new predators. Over two years (2013-14), 81 trout smolts were trapped and acoustically tagged 17 km upstream of the tidal limit in the River Frome in Dorset, UK. Smolt migrations were then tracked by acoustic receivers deployed throughout the lower river and its estuary and the detected movements were used to estimate loss rate and migration behaviour. A Bayesian State Space model was applied to separate detection and transition probabilities. More than 90% of the in-river detections occurred at night whereas detections at the saline limit and throughout the estuary were spread evenly between day and night. Median migration speed in the river was 65-70 km day-1 in both years, whereas the migration speed was slower through the estuary with median speeds of less than 10 km day-1. No zone displayed particularly high loss rates (range 0.5-1.1 % km-1), hence there was no pronounced increase in mortality in the transition zone as reported by a number of similar studies from other systems. The cumulative loss of trout smolts through the 33 km section that the smolts were tracked was 24%, demonstrating a significant cost in terms of loss associated with smolt migration.\nThe citation is: Lauridsen, R.B., Moore, A., Gregory, S.D., Beaumont, W.R.C., et al. (2017) Migration behaviour and loss rate of trout smolts in the transitional zone between freshwater and saltwater. Proceedings of the Second International Sea Trout Symposium, October 2015, Dundalk, Ireland.\nAnd a pdf reprint is available here: Lauridsen2017.pdf\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Estimating Sea Trout Smolt Migration Risks: Update},\n  date = {2016-10-14},\n  url = {https://stephendavidgregory.github.io/posts/holbrookii},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Estimating Sea Trout Smolt Migration\nRisks: Update.” October 14, 2016. https://stephendavidgregory.github.io/posts/holbrookii."
  },
  {
    "objectID": "posts/holbrookiii/index.html",
    "href": "posts/holbrookiii/index.html",
    "title": "Estimating sea trout smolt migration risks: talk",
    "section": "",
    "text": "I gave a talk recently at the Second IFM Tagging and Telemetry Workshop, held in Edinburgh in June 2016.\nMy slides are published here: Institute_of_Fisheries_Management_2nd_Tagging_and_Telemetry_Conference_2016.pdf.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Estimating Sea Trout Smolt Migration Risks: Talk},\n  date = {2016-10-14},\n  url = {https://stephendavidgregory.github.io/posts/holbrookiii},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Estimating Sea Trout Smolt Migration\nRisks: Talk.” October 14, 2016. https://stephendavidgregory.github.io/posts/holbrookiii."
  },
  {
    "objectID": "posts/jags-in-parallel/index.html",
    "href": "posts/jags-in-parallel/index.html",
    "title": "Running JAGS in parallel",
    "section": "",
    "text": "Sometimes, I run some pretty heavy duty models, including several latent variables and large datasets. I am also a perfectionist and so I insist on simulating and testing these models extensively.\nTo improve my efficiency, I can run Jags in parallel using a variety of tools, thereby saving considerable time.\n\nRunning Jags in parallel\nHere is a quick script I wrote to show a few different methods and how their timing compares on my (new and very fast) laptop.\n\n# startup\nrm(list = ls())\nrequire(dclone)\nrequire(rjags)\nrequire(foreach)\nrequire(doParallel)\nrequire(snow)\nrequire(snowfall)\nn.cores &lt;- 3\ntimings &lt;- vector('numeric', 6)\n\n# for easy plotting\nrequire(ggplot2)\n\n# MCMC settings\nsetts &lt;- list('n.iter' = 100, 'n.thin' = 1, 'n.burn' = 50)\nsetts.m &lt;- 1000\nmSetts &lt;- 1\nif(mSetts) setts &lt;- lapply(setts, function(v) v * setts.m)\nsetts$n.chains &lt;- 3\n\n# data\nn &lt;- 20\nx &lt;- runif(n, -1, 1)\nX &lt;- model.matrix(~ x)\nbeta &lt;- c(2, -1)\nmu &lt;- crossprod(t(X), beta)\nY &lt;- rpois(n, exp(mu))\ndat &lt;- list(Y = Y, X = X, n = n, np = ncol(X))\n\n# model\nglm.model &lt;- function() {\n  for (i in 1:n) {\n    Y[i] ~ dpois(lambda[i])\n    log(lambda[i]) &lt;- inprod(X[i,], beta[1, ])\n  }\n  for (j in 1:np) {\n    beta[1, j] ~ dnorm(0, 0.001)\n  }\n}\n\n# monitors; can add 'deviance' but left out here for easy plotting\nparams &lt;- c('beta')\n\n# fit with jags.fit\ntimer &lt;- proc.time()\nload.module('glm')\nload.module('lecuyer')\nload.module('dic')\nm0 &lt;- jags.fit(data = dat, params = params, model = glm.model,\n               n.chains = setts$n.chains, \n               n.adapt = 100, \n               n.update = setts$n.burn,\n               n.iter = setts$n.iter, \n               thin = setts$n.thin)\ntime.taken &lt;- proc.time() - timer\ntimings[1] &lt;- time.taken[3]\n\n# fit with jags.parfit\ntimer &lt;- proc.time()\ncl &lt;- makePSOCKcluster(n.cores)\ntmp &lt;- clusterEvalQ(cl, library(dclone))\nparLoadModule(cl, 'glm')\nparLoadModule(cl, 'lecuyer')\nparLoadModule(cl, 'dic')\nm1 &lt;- jags.parfit(cl = cl, data = dat, params = params, model = glm.model, \n                  n.chains = setts$n.chains, \n                  n.adapt = 100, \n                  n.update = setts$n.burn,\n                  n.iter = setts$n.iter, \n                  thin = setts$n.thin)\nstopCluster(cl)\ntime.taken &lt;- proc.time() - timer\ntimings[2] &lt;- time.taken[3]\n\n# fit with parJagsModel\ntimer &lt;- proc.time()\ncl &lt;- makePSOCKcluster(n.cores)\nparLoadModule(cl, 'glm')\nparLoadModule(cl, 'lecuyer')\nparLoadModule(cl, 'dic')\nparJagsModel(cl = cl, name = 'res', file = glm.model, data = dat,\n             n.chains = setts$n.chains, n.adapt = 100)\nparUpdate(cl = cl, object = 'res', n.iter = setts$n.burn)\nm2 &lt;- parCodaSamples(cl = cl, model = 'res', variable.names = params, \n                     n.iter = setts$n.iter, thin = setts$n.thin)\nstopCluster(cl)\ntime.taken &lt;- proc.time() - timer\ntimings[3] &lt;- time.taken[3]\n\n# fit with foreach\ntimer &lt;- proc.time()\ncl &lt;- makePSOCKcluster(n.cores)\nclusterSetRNGStream(cl)\nregisterDoParallel(cl)\nm3 &lt;- foreach(i = 1:setts$n.chains, .packages = c('dclone', 'rjags'),\n              .combine = 'c', .final = mcmc.list) %dopar% {\n                load.module('glm')\n                load.module('lecuyer')\n                load.module('dic')\n                m &lt;- jags.fit(data = dat, params = params, model = glm.model,\n                              n.chains = 1, \n                              n.adapt = 100, \n                              n.update = setts$n.burn,\n                              n.iter = setts$n.iter, \n                              thin = setts$n.thin, \n                              inits = list(.RNG.name = 'lecuyer::RngStream',\n                                           .RNG.seed = sample(1:1e6, 1)))\n              }\nstopCluster(cl)\ntime.taken &lt;- proc.time() - timer\ntimings[4] &lt;- time.taken[3]\n\n# fit with snow\ntimer &lt;- proc.time()\ncoda.samples.wrapper &lt;- function(i){ \n  load.module('glm')\n  load.module('lecuyer')\n  load.module('dic')\n  m &lt;- jags.fit(data = dat, params = params, model = glm.model,\n                n.chains = 1, \n                n.adapt = 100, \n                n.update = setts$n.burn,\n                n.iter = setts$n.iter, \n                thin = setts$n.thin, \n                inits = list(.RNG.name = 'lecuyer::RngStream',\n                             .RNG.seed = sample(1:1e6, 1)))\n}\ncl &lt;- makeCluster(n.cores, \"SOCK\")\nclusterEvalQ(cl, library('dclone'))\nclusterEvalQ(cl, library('rjags'))\nclusterExport(cl, list('dat', 'params', 'glm.model', 'setts'))\nm4 &lt;- clusterApply(cl, 1:setts$n.chains, coda.samples.wrapper)\nfor(i in 1:length(m4)){ # reorganize 'm4' as an 'mcmc.list' object\n  m4[[i]] &lt;- m4[[i]][[1]]\n}\nclass(m4) &lt;- \"mcmc.list\"\nstopCluster(cl)\ntime.taken &lt;- proc.time() - timer\ntimings[5] &lt;- time.taken[3]\n\n# fit with snowfall\ntimer &lt;- proc.time()\nsfInit(parallel = TRUE, cpus = n.cores)\nsfLibrary(rjags)\nsfLibrary(dclone)\nsfExportAll()\nm5 &lt;- sfLapply(1:setts$n.chains, function(i) {\n  load.module('glm')\n  load.module('lecuyer')\n  load.module('dic')\n  m &lt;- jags.fit(data = dat, params = params, model = glm.model,\n                n.chains = 1, \n                n.adapt = 100, \n                n.update = setts$n.burn,\n                n.iter = setts$n.iter, \n                thin = setts$n.thin, \n                inits = list(.RNG.name = 'lecuyer::RngStream',\n                             .RNG.seed = sample(1:1e6, 1)))\n})\nsfStop()\nfor(i in 1:length(m5)){ # reorganize 'm5' as an 'mcmc.list' object\n  m5[[i]] &lt;- m5[[i]][[1]]\n}\nclass(m5) &lt;- \"mcmc.list\"\ntime.taken &lt;- proc.time() - timer\ntimings[6] &lt;- time.taken[3]\n\n\n\n\n\n\n\n\n\n\n##         jags.fit      jags.parfit par... functions          foreach \n##            29.81            12.17            12.41            12.24 \n##             snow         snowfall \n##            12.42            12.32\nOf course, this would be better done using proper benchmarking procedures but I think it is interesting to note that there is no obvious differences in the timing of the different parallel procedures.\n\n\nMy preferences\nPersonally, I like dclone::par... functions for single-run models because I have become accustomed to using them (including using them to update particular parameters at different stages by keeping the clusters open) and I like snowfall for model testing when I want to run models many times over simulated datasets because it has fewer dependencies.\nI hope this is useful to someone.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {Running {JAGS} in Parallel},\n  date = {2017-04-16},\n  url = {https://stephendavidgregory.github.io/posts/jags-in-parallel},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “Running JAGS in Parallel.”\nApril 16, 2017. https://stephendavidgregory.github.io/posts/jags-in-parallel."
  },
  {
    "objectID": "posts/least-cost-dist-along-river/index.html",
    "href": "posts/least-cost-dist-along-river/index.html",
    "title": "Calculate distances along a river in R and GRASS",
    "section": "",
    "text": "Recently, I have been helping Jessica E. Marsh to develop a method of calculating distances along linear features. Specifically, we were measuring the distances between salmonid redd locations and juvenile survey locations on the river Frome, Dorset, UK.\nTogether, we devise a generalisable method to calculate the least cost distance between points located on a complex river network.\nOur method includes the following steps:\n\nconvert the river polygon dataset to a high resolution raster;\nthin that high resolution raster to represent the river as a network of single pixel width linear features;\nshift the sample site coordinates to the closest river pixel in the thinned raster; and\ncalculate the least cost distance between a set of coordinates.\n\nOur analysis is quite Frome-specific and so rather than outline the whole procedure Jess and I decided to present a contrived example to illustrate the last and most important step of the analysis: step 4.\nFor the remainder of the steps, we used a combination of R and GRASS GIS\n\nAn example\nWe provide a small example script with reproducible data (see downloads below) to run step 4.\nIt calculates distances along the local roads from the FBA River Laboratory (our address) to the local shop and pub.\n\n\nR script\n\n# start clean\nrm(list = ls())\n\n# start the clock\ntimer &lt;- proc.time()\n\n# libraries\nrequire(rgdal)\nrequire(raster)\nrequire(rasterVis)\nrequire(gdistance)\n\n# read in EastStoke to Wool road network; in your current path\nv &lt;- readOGR('EastStoke_to_Wool_roads.shp', 'EastStoke_to_Wool_roads')\n\n# add \"val\" field as 1 everywhere\nv@data$val &lt;- 1\n\n# get extent\nex &lt;- extent(v)\n\n# make empty raster at 1m resolution\nr &lt;- raster(ex, res = 0.0001, crs = proj4string(v))\n\n# rasterize road network\nrp &lt;- rasterize(v, r, 'val')\n\n# read in stops\nstops &lt;- readOGR('esw.shp')\n\n# find least cost distances for stops\ntr &lt;- transition(rp, function(x) 1 / mean(x), 8)\ntr1 &lt;- geoCorrection(tr)\nsl &lt;- shortestPath(tr1, origin = stops[1, ], goal = stops[-1, ], output = 'SpatialLines')\n\n# prepare plot\n# x11(width = 16, height = 10)\npar(mfrow = c(1, length(sl)))\n\n# plot path loop\nfor(i in 1:length(sl)){\n\n  ## plot raster\n  plot(rp, legend = FALSE)\n  \n  ## add road network\n  plot(v, add = TRUE)\n\n  ## add stops to plot\n  points(stops[c(1, i + 1), ], pch = 16, col = 'red')\n\n  ## add shortest path\n  plot(sl[i], col = 'blue', add = TRUE)\n}\n\n# get path lengths\nl &lt;- SpatialLinesLengths(sl)\nprint(l)\nDownload the data here.\nThe final result is shown on a map at the start of this post.\nYou can check the calculations against those given by Google Maps here\nWe hope this is of interest to someone. If you have any questions, then contact Jess Marsh or I by email.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {Calculate Distances Along a River in {R} and {GRASS}},\n  date = {2017-01-31},\n  url = {https://stephendavidgregory.github.io/posts/least-cost-dist-along-river},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “Calculate Distances Along a River in R\nand GRASS.” January 31, 2017. https://stephendavidgregory.github.io/posts/least-cost-dist-along-river."
  },
  {
    "objectID": "posts/mixed-logistic-regression/index.html",
    "href": "posts/mixed-logistic-regression/index.html",
    "title": "(Mixed) Logistic regression in JAGS",
    "section": "",
    "text": "Here is a quick post showing how to do a fixed effect logistic regression in jags, extend it to a mixed effects logistic regression and show how it can be fitted using two contrasting specifications, each giving the same result (within stochastic error).\nPerhaps these models will work in *BUGS variants but I tend to prefer jags because it is platform-independent, more similar to R, actively developed and with excellent support (thanks, Martin!) and it works nicely with a number of R packages, including dclone. dclone facilitates running jags, particularly in parallel, and is also under continued development (thanks, Peter!).\nI will soon write a blog post presenting a template I use to run analyses in jags using dclone, in case it is useful.\n\nA simple fixed effect logistic regression\nBefore delving into mixed effects logistic regression, I thought it would be a good idea (and reassuring) to fit a simple fixed effect logistic regression in R and jags.\nHere is the script:\nrequire(dclone)\nrequire(lme4)\n\n# model\nm &lt;- function(){\n  for (i in 1:N){\n    y[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- a + b * x[i]\n  }\n  a ~ dnorm(0, 0.01)\n  b ~ dnorm(0, 0.01)\n}\n\n# data\nN &lt;- 1000\nx &lt;- rnorm(N)\na &lt;- 1\nb &lt;- 2\nz &lt;- a + b * x\npr &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(N, 1, pr)\ndf &lt;- data.frame(y = y, x = x)\n\n# glm fit\nm0 &lt;- glm(y ~ x, data = df, family = 'binomial')\nprint(m0)\n## \n## Call:  glm(formula = y ~ x, family = \"binomial\", data = df)\n## \n## Coefficients:\n## (Intercept)            x  \n##       1.056        2.093  \n## \n## Degrees of Freedom: 999 Total (i.e. Null);  998 Residual\n## Null Deviance:       1278 \n## Residual Deviance: 838.9     AIC: 842.9\n# jags data\ndat &lt;- as.list(df)\ndat$N &lt;- N\n\n# monitors\nparams &lt;- c('a', 'b')\n\n# run it\nout &lt;- jagsModel(file = m, data = dat, n.chains = 3, n.adapt = 1e3)\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 1000\n##    Unobserved stochastic nodes: 2\n##    Total graph size: 5007\n## \n## Initializing model\nupdate(out, n.iter = 1e3)\ns &lt;- codaSamples(out, variable.names = params, n.iter = 1e4, n.thin = 1e2)\ns_sum &lt;- summary(s)\n\n# print medians\nprint(s_sum$statistics)\n##       Mean        SD     Naive SE Time-series SE\n## a 1.058522 0.0940543 0.0005430228   0.0008407071\n## b 2.101140 0.1404204 0.0008107177   0.0012805193\nprint(s_sum$quantiles)\n##        2.5%       25%      50%      75%    97.5%\n## a 0.8778516 0.9944358 1.057302 1.121390 1.245571\n## b 1.8307969 2.0055994 2.098017 2.193111 2.388057\nComparing the glm and jags results shows that the estimates are very similar, suggesting that the jags model is well-specified.\nSo, let’s move on to the mixed effects logistic regression…\n\n\nA mixed effect logistic regression\nI fit a mixed effect logistic regression with a random effect of group specified as a error term or as an intercept term.\nrequire(dclone)\nrequire(lme4)\n\n# models\nm_epsilon &lt;- function(){\n  for (i in 1:N){\n    y[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- a + (b * x[i]) + epsilon[g[i]]\n  }\n  for(j in 1:G){\n    epsilon[j] ~ dnorm(0, tau)\n  }\n  tau ~ dgamma(0.1, 0.1)\n  std_dev &lt;- tau^-0.5\n  variance &lt;- std_dev^2\n  a ~ dnorm(0, 0.01)\n  mean_phi &lt;- 1 / (1 + exp(-a))\n  b ~ dnorm(0, 0.01)\n}\n\nm_intercept &lt;- function(){\n  for (i in 1:N){\n    y[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- a_[g[i]] + b * x[i]\n  }\n  for(j in 1:G){\n    a_[j] ~ dnorm(a, tau)\n  }\n  tau ~ dgamma(0.1, 0.1)\n  std_dev &lt;- tau^-0.5\n  variance &lt;- std_dev^2\n  a ~ dnorm(0, 0.01)\n  mean_phi &lt;- 1 / (1 + exp(-a))\n  b ~ dnorm(0, 0.01)\n}\n\n# data\nn &lt;- 100\nG &lt;- 10\ng &lt;- gl(G, n)\nN &lt;- n * G\nx &lt;- rnorm(N)\nXmat &lt;- model.matrix(~ g + x - 1)\nmu &lt;- 1\nmean_phi &lt;- 1 / (1 + exp(-mu))\nsd &lt;- 0.5\na &lt;- rnorm(G, mu, sd)\nb &lt;- 2\nab &lt;- c(a, b)\nz &lt;- Xmat %*% ab\npr &lt;- 1 / (1 + exp(-z))\ny &lt;- rbinom(N, 1, pr)\ndf &lt;- data.frame(y = y, g = g, x = x)\n\n# glmer fit\nm0 &lt;- glmer(y ~ x + (1|g), data = df, family = 'binomial')\nprint(m0)\n## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: y ~ x + (1 | g)\n##    Data: df\n##       AIC       BIC    logLik  deviance  df.resid \n##  834.0712  848.7945 -414.0356  828.0712       997 \n## Random effects:\n##  Groups Name        Std.Dev.\n##  g      (Intercept) 0.586   \n## Number of obs: 1000, groups:  g, 10\n## Fixed Effects:\n## (Intercept)            x  \n##       1.095        2.118\nprint(exp(round(ranef(m0)$g, 3)))\n##    (Intercept)\n## 1    1.3391030\n## 2    0.5347264\n## 3    1.2788996\n## 4    2.6090861\n## 5    0.6250023\n## 6    0.4073836\n## 7    1.2226248\n## 8    0.9617507\n## 9    0.8411376\n## 10   1.6015950\n# jags data\ndat &lt;- as.list(df)\ndat$N &lt;- N\ndat$G &lt;- G\n\n# monitors\nparams &lt;- c('a', 'b', 'mean_phi')\n\n# run it\nout &lt;- jagsModel(file = m_epsilon, data = dat, n.chains = 3, n.adapt = 1e3)\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 1000\n##    Unobserved stochastic nodes: 13\n##    Total graph size: 6047\n## \n## Initializing model\nupdate(out, n.iter = 1e3)\ns_epsilon &lt;- codaSamples(out, variable.names = params, n.iter = 1e4, n.thin = 1e2)\ns_epsilon_sum &lt;- summary(s_epsilon)\n\n# print medians\nprint(s_epsilon_sum$statistics)\n##              Mean         SD     Naive SE Time-series SE\n## a        1.091157 0.24343847 0.0014054927    0.006733634\n## b        2.130777 0.14122313 0.0008153521    0.001298608\n## mean_phi 0.745888 0.04591976 0.0002651179    0.001266695\nprint(s_epsilon_sum$quantiles)\n##               2.5%       25%       50%       75%     97.5%\n## a        0.6043638 0.9365635 1.0935534 1.2476852 1.5835063\n## b        1.8630997 2.0346300 2.1280164 2.2235364 2.4152579\n## mean_phi 0.6466540 0.7184050 0.7490503 0.7768989 0.8297005\n# run it\nout &lt;- jagsModel(file = m_intercept, data = dat, n.chains = 3, n.adapt = 1e3)\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 1000\n##    Unobserved stochastic nodes: 13\n##    Total graph size: 6037\n## \n## Initializing model\nupdate(out, n.iter = 1e3)\ns_intercept &lt;- codaSamples(out, variable.names = params, n.iter = 1e4, n.thin = 1e2)\ns_intercept_sum &lt;- summary(s_intercept)\n\n# print medians\nprint(s_intercept_sum$statistics)\n##               Mean         SD     Naive SE Time-series SE\n## a        1.0996923 0.24700895 0.0014261068    0.001824264\n## b        2.1323963 0.13937092 0.0008046584    0.001362175\n## mean_phi 0.7474055 0.04612672 0.0002663127    0.000340099\nprint(s_intercept_sum$quantiles)\n##               2.5%       25%       50%       75%     97.5%\n## a        0.6111628 0.9446477 1.0958199 1.2533071 1.5996372\n## b        1.8698045 2.0363684 2.1291373 2.2254063 2.4140774\n## mean_phi 0.6482060 0.7200375 0.7494761 0.7778718 0.8319677\nBased on the statistics and quantiles of the MCMC samples, I hope you can see that both the lme4 and Bayesian fits are similar, and that the Bayesian results with group random effect specified as a error term or as an intercept term also produce similar results.\nI hope this is useful to someone.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {(Mixed) {Logistic} Regression in {JAGS}},\n  date = {2017-05-16},\n  url = {https://stephendavidgregory.github.io/posts/mixed-logistic-regression},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “(Mixed) Logistic Regression in\nJAGS.” May 16, 2017. https://stephendavidgregory.github.io/posts/mixed-logistic-regression."
  },
  {
    "objectID": "posts/ordered-factors/index.html",
    "href": "posts/ordered-factors/index.html",
    "title": "Ordered factors and their analysis",
    "section": "",
    "text": "What is an ordered factor?\n\n\nWhy might we use them?\n\n\nAnd what are the implications of using ordered factors in statistical analysis?\n\nIf you’d like my views on these questions, then read on…\n\nBackground\n\nWhat is an ordered factor?\n\nAn ordered factor is a categorical variable whose levels have a meaningful order. In the oven knob picture above, there are 4 levels: ‘OFF’, ‘LOW’, ‘MED’ and ‘HIGH’ and these should be ordered as ‘OFF &lt; LOW &lt; MED &lt; HIGH’.\n\nWhy might we use them?\n\nWe often choose to use an ordered factor when it is difficult to put our variable on a meaningful continuous scale, i.e., from \\(-\\infty\\) to \\(+\\infty\\) where all numbers are real, e.g., not an integer or a qualitative representation of a real number, which would be a category.\nSometimes, we choose to use an ordered factor because we don’t have the equipment required to measure a variable on a continuous scale, e.g., river flow might be classified on a ordinal scale of ‘1 &lt; 2 &lt; 3 &lt; 4 &lt; 5’, which has 5 levels. This is not, however, a continuous variable.\n\nAnd what are the implications of using ordered factors in statistical analysis?\n\nWhen we choose to classify a continuous variable on an ordinal scale, i.e., treat the variable as a ordered factor, then we should be aware of the statistical implications for doing so. In the situation that we have a small sample size, i.e., a small number of cases, then we will likely have only few residual degrees of freedom to assess the ‘statistical significance’ of our statistical model. This is because we can only assess the ‘statistical significance’ of an ordered factor by fitting it as a polynomial term with the degree equal to the number of levels - 1. In our river flow example, the effect of river flow x on our response variable y is assessed by fitting the polynomial contrasts x^1, x^2, x^3 and x^4 (but not x^5 because one level has to be retained as the contrast level, hence the polynomial term has number of levels - 1 degrees).\nIn what follows, I will try to illustrate these points using simulated data in R. I present all the code so that the interested reader can run the analyses for themselves.\n\n\nStatistical analysis of an ordered factor\nLet’s begin by simulating some data for the multiple regression\n\\[y \\sim \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\]\nIn this example, \\(x_1\\) might be water temperature and \\(x_2\\) might be river flow, both recorded on their (theoretically) continuous scale \\(-\\infty &lt; 0 &lt; +\\infty\\). (In actual fact, river flow would be a Gamma variate because it can not be less than zero.)\nOnce we have simulated the data, let us fit the generating model back to the simulated data to show that we have done it correctly, and then plot the partial (or marginal) effects of each explanatory variable (Figure 1).\n# load libraries\nrequire(effects)\n\n# simulate data; generating values\nN &lt;- 1000\na &lt;- 5\nb1 &lt;- 5\nb2 &lt;- 3\nx1 &lt;- rnorm(N)\nx2 &lt;- rnorm(N)\nsigma &lt;- 1\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- a + (b1 * x1) + (b2 * x2) + epsilon\n\n# fit lm()\nlm_fit &lt;- summary(lm(y ~ x1 + x2))\n\n# print coefs; note they are similar to the generating values\nround(lm_fit$coef, 3)\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)    5.031      0.031 163.066        0\n## x1             5.023      0.031 161.188        0\n## x2             3.052      0.031 100.010        0\n# print sigma; note it is similar to its generating value\nround(lm_fit$sigma, 3)\n## [1] 0.975\n# plot\nfoo &lt;- lm(y ~ x1 + x2)\nfii &lt;- allEffects(mod = foo, default.levels = 50)\nplot(fii)\n\n\n\nFigure 1. Fitting the generating model to the simulated data - success!\n\n\nNow, what would happen if \\(x_2\\) had been recorded on the ordinal scale \\(1 &lt; 2 &lt; 3 &lt; 4 &lt; 5\\)?\n# discretize x2\nnLevels &lt;- 5\nx2d &lt;- cut(x = x2, breaks = nLevels, labels = 1:5, ordered_result = TRUE)\nstr(x2d) # ordered factor with nLevels = 5 recorded levels\n##  Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 3 3 2 3 3 3 3 3 4 4 ...\n# fit discretized lm() with linear effect of x2d (coef name is x2d.L)\nlm_fit_d &lt;- summary(lm(y ~ x1 + x2d))\n\n# \"x2d.L\" is significant linear effect\n# note that lm() estimates nLevels - 1 (in this case, 5 - 1 = 4) coefs for the effect of x2d\nround(lm_fit_d$coef, 3)\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)    5.491      0.111  49.650    0.000\n## x1             4.978      0.049 101.562    0.000\n## x2d.L         11.975      0.338  35.457    0.000\n## x2d.Q          0.436      0.288   1.516    0.130\n## x2d.C          0.020      0.191   0.105    0.916\n## x2d^4          0.011      0.106   0.101    0.919\nround(lm_fit_d$sigma, 3)\n## [1] 1.531\n# plot\nfoo &lt;- lm(y ~ x1 + x2d)\nfii &lt;- allEffects(mod = foo, default.levels = 50)\nplot(fii)\n\n\n\nFigure 2. When treating the continuous variable as an ordered factor.\n\n\nFrom the partial effect plots, we see that the influence of \\(x_1\\) and \\(x_2\\) on \\(y\\) are very similar whether \\(x_2\\) is treated as a continuous variable or an ordered factor.\nNote, however, that the statistical model in which \\(x_2\\) is treated as an ordered factor has three additional coefficient estimates, representing the polynomial contrasts \\(x_2^2\\), \\(x_2^3\\) and \\(x_2^4\\). And the partial effect plot of \\(x_2\\) (x2d) on \\(y\\) is represented as a set of contrasts.\nNote also, however, that our estimate of x2d.L (11.975 rather than 3) and sigma (1.531 rather than 1) is poor, reflecting the fact that we have not captured the true effect of \\(x_2\\) on \\(y\\) by treating the continuous variable \\(x_2\\) as an ordered factor.\n\n\nA more correct and flexible illustration\nTo overcome the problem of treating the continuous variable \\(x_2\\) as an ordered factor, let us generate data for an example in which \\(x_2\\) is actually measured as an ordered factor.\nAlso, let us define sigLevel as the level of the ordered factor \\(x_2\\) that statistically influences \\(y\\).\n# set the level at which x2 has a significant effect on y\nsigLevel &lt;- 1 # a linear effect of x_2 on y, i.e., the significant polynomial contrast is x^1\n\n# simulate data\nN &lt;- 1000\nnLevels &lt;- 5\na &lt;- 5\nb1 &lt;- 5\nb2 &lt;- rep(0, nLevels - 1) # nLevels - 1 contrasts; level nLevel set as contrast\nb2[sigLevel] &lt;- 3\nx1 &lt;- rnorm(N)\nx2 &lt;- gl(n = nLevels, k = N / nLevels, ordered = TRUE)\nXmat &lt;- model.matrix(~ x1 + x2)\nbvec &lt;- c(a, b1, b2)\nlp &lt;- Xmat %*% bvec\nsigma &lt;- 1\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- as.numeric(lp + epsilon)\n\n# fit lm()\nlm_fit_d &lt;- summary(lm(y ~ x1 + x2))\nround(lm_fit_d$coef, 3)\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)    5.006      0.032 157.573    0.000\n## x1             5.000      0.031 161.650    0.000\n## x2.L           3.040      0.071  42.901    0.000\n## x2.Q          -0.039      0.071  -0.546    0.585\n## x2.C           0.048      0.071   0.673    0.501\n## x2^4          -0.049      0.071  -0.696    0.487\nround(lm_fit_d$sigma, 3)\n## [1] 1.002\n# plot\nfoo &lt;- lm(y ~ x1 + x2)\nfii &lt;- allEffects(mod = foo, default.levels = 50)\nplot(fii)\n\n\n\nFigure 3. When treating the continuous variable as an ordered factor.\n\n\nThat is better: the estimated coefficients for a, b1 and b2[sigLevel] and sigma are close to their generating values, and the plot shows a linear effect of \\(x_2\\) on \\(y\\).\nWe can change sigLevel so that the effect of \\(x_2\\) on \\(y\\) takes a different shape, e.g., a quadratic effect represented by the statistically significant polynomial contrast x^2, as follows:\n# what about a quadratic effect of x2 on y\nsigLevel &lt;- 2 # a quadratic effect of x_2 on y, i.e., the significant polynomial contrast is x^2\n\n# simulate data\nN &lt;- 1000\nnLevels &lt;- 5\na &lt;- 5\nb1 &lt;- 5\nb2 &lt;- rep(0, nLevels - 1) # nLevels - 1 contrasts; level nLevel set as contrast\nb2[sigLevel] &lt;- 3\nx1 &lt;- rnorm(N)\nx2 &lt;- gl(n = nLevels, k = N / nLevels, ordered = TRUE)\nXmat &lt;- model.matrix(~ x1 + x2)\nbvec &lt;- c(a, b1, b2)\nlp &lt;- Xmat %*% bvec\nsigma &lt;- 1\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- as.numeric(lp + epsilon)\n\n# fit lm()\nlm_fit_d &lt;- summary(lm(y ~ x1 + x2))\nround(lm_fit_d$coef, 3)\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)    5.003      0.031 163.497    0.000\n## x1             5.007      0.029 170.784    0.000\n## x2.L          -0.148      0.068  -2.161    0.031\n## x2.Q           2.974      0.068  43.470    0.000\n## x2.C           0.002      0.068   0.031    0.975\n## x2^4          -0.057      0.068  -0.831    0.406\nround(lm_fit_d$sigma, 3)\n## [1] 0.967\n# plot\nfoo &lt;- lm(y ~ x1 + x2)\nfii &lt;- allEffects(mod = foo, default.levels = 50)\nplot(fii)\n\n\n\nFigure 4. When treating the continuous variable as an ordered factor.\n\n\nAgain, note that the coefficient and sigma estimates are close to their generating values: success!\n\n\nThe problem of sample size…\n\nAnd what are the implications of using ordered factors in statistical analysis?\n\nIn the background, I mentioned that the implications for choosing to use an ordered factor in place of a continuous variable will be particularly important when the sample size (or number of cases) is low.\nThis is because any statistical analysis of an ordered factor will require several contrasts (specifically, number of levels - 1 contrasts) and that those degrees of freedom are no longer “free” to assess the statistical significance of the model. This becomes particularly problematic when the number of cases is low, as illustrated for the simple linear effect of ordered factor \\(x_2\\) on \\(y\\) simulated here…\n# what about a quadratic effect of x2 on y\nsigLevel &lt;- 1 # a linear effect of x_2 on y, i.e., the significant polynomial contrast is x^1\n\n# common generating values\nN &lt;- 7\nnLevels &lt;- 5\na &lt;- 5\nb1 &lt;- 5\nx1 &lt;- rnorm(N)\nsigma &lt;- 0.25 # reduced to facilitate illustration\n\n# simulate data for continous variable\nb2 &lt;- 3\nx2c &lt;- rnorm(N)\nXmat &lt;- model.matrix(~ x1 + x2c)\nbvec &lt;- c(a, b1, b2)\nlp &lt;- Xmat %*% bvec\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- as.numeric(lp + epsilon)\n\n# fit lm() and print model performance measures\nlm_fit_c &lt;- summary(lm(y ~ x1 + x2c))\nlm_fit_c\n## \n## Call:\n## lm(formula = y ~ x1 + x2c)\n## \n## Residuals:\n##        1        2        3        4        5        6        7 \n## -0.04064 -0.04331 -0.06432 -0.03548 -0.04572  0.02031  0.20916 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  5.08128    0.04750  106.98 4.58e-08 ***\n## x1           4.81239    0.09851   48.85 1.05e-06 ***\n## x2c          2.91798    0.09785   29.82 7.53e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1174 on 4 degrees of freedom\n## Multiple R-squared:  0.9986, Adjusted R-squared:  0.9978 \n## F-statistic:  1386 on 2 and 4 DF,  p-value: 2.076e-06\n# simulate data for ordered factor\nb2 &lt;- rep(0, nLevels - 1) # nLevels - 1 contrasts; level nLevel set as contrast\nb2[sigLevel] &lt;- 3\nx2d &lt;- gl(n = nLevels, k = N / nLevels, ordered = TRUE)\nXmat &lt;- model.matrix(~ x1 + x2d)\nbvec &lt;- c(a, b1, b2)\nlp &lt;- Xmat %*% bvec\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- as.numeric(lp + epsilon)\n\n# fit lm() and print model performance measures\nlm_fit_d &lt;- summary(lm(y ~ x1 + x2d))\nlm_fit_d\n## \n## Call:\n## lm(formula = y ~ x1 + x2d)\n## \n## Residuals:\n##          1          2          3          4          5          6 \n## -3.388e-02  6.715e-02  1.214e-17  3.469e-18  1.908e-17  3.388e-02 \n##          7 \n## -6.715e-02 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)  4.98801    0.04368 114.183  0.00558 **\n## x1           4.76806    0.16777  28.420  0.02239 * \n## x2d.L        2.96444    0.15136  19.586  0.03248 * \n## x2d.Q       -0.44414    0.09805  -4.530  0.13832   \n## x2d.C       -0.19028    0.10317  -1.844  0.31628   \n## x2d^4        0.02820    0.12754   0.221  0.86148   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1064 on 1 degrees of freedom\n## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9991 \n## F-statistic:  1355 on 5 and 1 DF,  p-value: 0.02062\nThe Multiple R-squared and Adjusted R-squared measures are very similar for both models but the statistical significance of the models are very different (continuous \\(\\approx\\) 0; ordered factor = 0.021).\nNote also that the coefficients estimates for the different explanatory variables have also dropped in “statistical significance” and the effect of \\(x_2\\) is now negligible.\n\n\nDon’t treat an ordered factor as continuous!\nIt has been suggested that one can treat an ordered factor as a continuous variable to avoid this problem. Here is how that might work:\n# what about a quadratic effect of x2 on y\nsigLevel &lt;- 1 # a linear effect of x_2 on y, i.e., the significant polynomial contrast is x^1\n\n# common generating values\nN &lt;- 1000\nnLevels &lt;- 5\na &lt;- 5\nb1 &lt;- 5\nx1 &lt;- rnorm(N)\nb2 &lt;- rep(0, nLevels - 1) # nLevels - 1 contrasts; level nLevel set as contrast\nb2[sigLevel] &lt;- 3\nx2d &lt;- gl(n = nLevels, k = N / nLevels, ordered = TRUE)\nXmat &lt;- model.matrix(~ x1 + x2d)\nbvec &lt;- c(a, b1, b2)\nlp &lt;- Xmat %*% bvec\nsigma &lt;- 0.25 # reduced to facilitate illustration\nepsilon &lt;- rnorm(N, 0, sigma)\ny &lt;- as.numeric(lp + epsilon)\n\n# fit lm() and print model performance measures\nlm_fit_d &lt;- summary(lm(y ~ x1 + x2d))\nround(lm_fit_d$coef, 3)\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)    4.985      0.008 628.050    0.000\n## x1             5.016      0.008 634.895    0.000\n## x2d.L          3.004      0.018 169.452    0.000\n## x2d.Q         -0.013      0.018  -0.733    0.464\n## x2d.C         -0.003      0.018  -0.190    0.849\n## x2d^4          0.004      0.018   0.251    0.802\n# note that this can be fitted as a poly() and treating x2 as continuous\nx2c &lt;- as.numeric(as.character(x2d))\nlm_fit_dc &lt;- summary(lm(y ~ x1 + x2c))\nround(lm_fit_dc$coef, 3)\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)    2.135      0.019 114.950        0\n## x1             5.015      0.008 636.054        0\n## x2c            0.950      0.006 169.653        0\nIn this case, however, the estimated effect of \\(x_2\\) on \\(y\\) is meaningless; it is estimated as 0.95 rather than the real value of 3. In other words, it might be “statistically significant” but no-one has any idea what the effect size is!\nMoreover, it changes the coefficient estimates of other estimated parameters, in this case a.\n\n\nMy views\nRecording a continous variable as an ordered factor might seem like an easy solution when faced with limited resources, such as time or money.\nI warn, however, that before taking that decision, consider the following issues carefully:\n\nWhat will be my sample size or number of cases? If the number of cases is low, particularly if the number of cases per explanatory variable is fewer than 5, then one really should quantify the variable on its true continuous scale, irrespective of the resources available.\nIf using an ordinal scale, how does it relate to the continuous scale? Perhaps one can get away with using an ordinal scale if it is very carefully designed to capture the range of values (and therefore effect sizes) of the continuous variable on the response variable.\nCan one use a different variable Perhaps there is a better explanatory variable to use?\nThink carefully!\n\nI hope this is useful to someone.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {Ordered Factors and Their Analysis},\n  date = {2017-03-17},\n  url = {https://stephendavidgregory.github.io/posts/ordered-factors},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “Ordered Factors and Their\nAnalysis.” March 17, 2017. https://stephendavidgregory.github.io/posts/ordered-factors."
  },
  {
    "objectID": "posts/parr-lengths-paper-published/index.html",
    "href": "posts/parr-lengths-paper-published/index.html",
    "title": "New paper: Scale-dependent drivers of Atlantic salmon parr lengths",
    "section": "",
    "text": "New paper: scale-dependent drivers of Atlantic salmon parr lengths\nOur paper “Patterns on a parr: drivers of long-term salmon parr length in UK and French rivers depends on geographical scale” is out today in Freshwater Biology!\nGregory SD, Nevoux M, Riley WD, Beaumont WRC, Jeannot N, Lauridsen RB, Marchand F, Scott LJ, and Roussel J-M. (2017) Patterns on a parr: drivers of long-term salmon parr length in UK and French rivers depends on geographical scale. Freshw Biol. https://doi.org/10.1111/eff.12929\n\nAbstract\n\nUnderstanding the geographical scales at which environmental variables affect an individual’s body size, and thus their mortality risk, can inform management strategies to help conserve wild populations under climate change. Yet, our current understanding of these relationships is based on studies done at different scales that report inconsistent findings. We predicted that temperature-related variables (e.g. winter temperature) influence body size at a “regional” scale, that is, affecting individuals in geographically independent catchments similarly, whereas non-temperature-related variables (e.g. conspecific competitor density) exert a “local” influence, that is, affecting individuals in geographically independent catchments differently.\nWe developed statistical models to test our predictions using body length measures of a large and long-term sample of juvenile Atlantic salmon (Salmo salar) from three rivers in the U.K. and France. We developed mixture models to predict the individual juvenile salmon ages objectively from their body length. We then developed linear mixed models to describe inter-annual changes in mean length of the youngest (age 0) cohort of juvenile salmon from river-specific seasonal variables, and tested whether they exerted their influence at a “local” or “regional” scale. All models accounted for spatio-temporal differences in sampling protocols and individual reproductive strategy. We estimated and interpreted coefficients using Bayesian theory.\nOur findings supported our predictions. Juvenile salmon were longer in years of higher overwinter water temperature and this effect was best parameterised as a single “regional” coefficient applicable to all three rivers. Similarly, spring mean temperature was best parameterised with a single “regional” nonlinear coefficient. In contrast, juvenile salmon were shorter in years of high densities of competing conspecifics and their interaction with total mean discharge and these effects were represented by local river-specific coefficients. Summer droughts had a negative effect on juvenile salmon length but was best parameterised as a single “regional” coefficient, contrary to our expectations.\nWe show that environmental variables affect biological processes at different but predictable geographical scales. Temperature-related variables affect body sizes of exothermic animals at a regional scale, whereas non-temperature variables, such as the density of conspecific competitors and water abstraction, exert their influence at a local scale. These findings highlight the importance of integrating local and regional management plans to mitigate the impacts of climate change on the body size, and ultimately the conservation, of exothermic species.\n\nA pdf reprint is available here: Gregory2017.pdf\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {New Paper: {Scale-dependent} Drivers of {Atlantic} Salmon\n    Parr Lengths},\n  date = {2017-04-17},\n  url = {https://stephendavidgregory.github.io/posts/parr-lengths-paper-published},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “New Paper: Scale-Dependent Drivers of\nAtlantic Salmon Parr Lengths.” April 17, 2017. https://stephendavidgregory.github.io/posts/parr-lengths-paper-published."
  },
  {
    "objectID": "posts/parry-published/index.html",
    "href": "posts/parry-published/index.html",
    "title": "New paper: Flow and Atlantic salmon redd distribution",
    "section": "",
    "text": "Our paper “The effects of flow on Atlantic salmon (Salmo salar) redd distribution in a UK chalk stream between 1980 and 2015” is out today in Ecology of Freshwater Fish!\nParry ES, Gregory SD, Lauridsen RB, and Griffiths SW. The effects of flow on Atlantic salmon (Salmo salar) redd distribution in a UK chalk stream between 1980 and 2015. Ecol Freshw Fish. 2017;00:1–10. https://doi.org/10.1111/eff.12330\n\nAbstract\nAtlantic salmon are an ecologically and economically important migratory fish in the UK, whose stocks have been declining over the past 30 years. Future climate and water use changes have the potential to alter the reproductive behaviour and distribution of salmon within a river, by restricting times and ability to access suitable spawning areas. As the survival of emergent salmon juveniles is density dependent, understanding how climate-driven changes in flow affect the location of salmon redds is important for future conservation efforts. This study examined how flow conditions affect the distribution of redds within a UK chalk stream, the river Frome in Dorset. Sixteen years of redd distribution and flow data between 1980 and 2015 were analysed using linear mixed-effects modelling. Generally, highest redd densities occurred within middle reaches of the main river. Mean flow during the river Frome critical migration period (October–December) did not affect the density of redds directly but affected the relationship between redd density and distance from tidal limit: redd densities were spread more uniformly throughout the river under high flow conditions, whereas redds were more aggregated in the middle river reaches under low flow conditions. Together, these findings suggest that access to upstream spawning grounds was limited under low flow conditions, which could have negative repercussions on juvenile survival. This study has revealed the distribution of redds along the river Frome for the first time and provided a basis for further study into the effects of redd distribution on subsequent juvenile life stages.\nA pdf reprint is available here: Parry2018.pdf\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {New Paper: {Flow} and {Atlantic} Salmon Redd Distribution},\n  date = {2017-01-17},\n  url = {https://stephendavidgregory.github.io/posts/parry-published},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “New Paper: Flow and Atlantic Salmon\nRedd Distribution.” January 17, 2017. https://stephendavidgregory.github.io/posts/parry-published."
  },
  {
    "objectID": "posts/pattern-on-a-parr/index.html",
    "href": "posts/pattern-on-a-parr/index.html",
    "title": "Patterns on a parr: talk",
    "section": "",
    "text": "Here is a link to the Tuesday research seminar I gave to the Game and Wildlife Conservation Trust, at Fordingbridge on 24th January 2017.\nSlides at [coming soon!]\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {Patterns on a Parr: Talk},\n  date = {2017-01-17},\n  url = {https://stephendavidgregory.github.io/posts/pattern-on-a-parr},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “Patterns on a Parr: Talk.”\nJanuary 17, 2017. https://stephendavidgregory.github.io/posts/pattern-on-a-parr."
  },
  {
    "objectID": "posts/quadratic-interaction-jags/index.html",
    "href": "posts/quadratic-interaction-jags/index.html",
    "title": "Quadratic interaction terms fitted by Bayesian Variable Selection",
    "section": "",
    "text": "I was recently faced with the challenge of testing the importance of non-linear effects in a linear mixed model with fixed effect interaction terms. This was an extension of previous work in which I fitted a linear mixed model and tested the importance of interacion terms using Bayesian Variable Selection."
  },
  {
    "objectID": "posts/quadratic-interaction-jags/index.html#notes",
    "href": "posts/quadratic-interaction-jags/index.html#notes",
    "title": "Quadratic interaction terms fitted by Bayesian Variable Selection",
    "section": "Notes",
    "text": "Notes\n\nthis is dependent on iLI: “if not interaction, then try linear” (not “if linear, then try interaction”)\nthis is dependent on iLM: “if linear, then try quadratic” (not “if quadratic, then keep linear”)\nthis is independent of all other terms: “try linear interaction”\nthis is dependent on iLM: “if linear interaction, then try quadratic interaction” (not “if quadratic interaction, then keep linear interaction”)"
  },
  {
    "objectID": "posts/salmonids-in-floods/index.html",
    "href": "posts/salmonids-in-floods/index.html",
    "title": "Salmonids in the floods",
    "section": "",
    "text": "As the rate of climate change accelerates, so too does the rate at which climate-related records are surpassed. The 2015/2016 winter has gone down in history as the second wettest UK winter since records began, beaten only by the 2013/2014 winter. January 2014 was the wettest month in almost 250 years. This followed an appallingly soggy 2012 that was (at the time) the second wettest year on record. Some might seek solace that the wettest year on record was decades ago. Sadly not: the wettest year on record was in 2000.\nTo cut a long story short, four of the UK’s five wettest recorded years have occurred since 2000 (in order of wetness: 2000, 2012, 2008 and 2002).\nIt seems that long-term woodland destruction, urbanisation, loss of lakes and ponds and modifications to natural river channels have literally cleared the way for a torrent of water to cascade unabated into lower river catchments. And this happens so quickly that there isn’t time for drainage systems to cope. Flooding ensues and endures.\nWhilst their effect on human inhabitations is all too obvious, we are not the only species to be affected by floods. Fish inhabit the very rivers that are bursting their banks onto the surrounding towns and countryside. Unlike humans, however, fish have been experiencing floods in their native river habitats for hundreds or thousands of years. And because floods present a strong evolutionarily selective force (you adapt or you die), fish have adapted to survive and even thrive under certain flood conditions.\nSalmonids are a good example of a group of fish species that have adapted to local flow regimes, inhabiting a range of rivers from those in Scotland with highly variable flows and high-gradient channels to shallow and stable slow-flowing chalk stream in southern England.\nUnderstanding how floods affect salmon is complicated by three unavoidable factors:\n\nFloods will affect different salmon life stages, such as eggs and adults, differently\nFloods of different characteristics, such as timing and duration, will affect each life stage differently\nOur ability to study salmon movements and behaviours under flood conditions is impacted by turbid waters and reduced equipment efficiency or malfunction\n\nThe interactions between the first two complexities have not been well studied, perhaps because floods are rare events and studies to understand the effects of flood characteristics on different salmon life stages will suffer from too few observations. Instead, research has tended to examine the influence of a single measure of flow on a specific salmon life stage and its variation.\nA factor that limits the distribution of salmonids is availability of useable spawning gravels. One can imagine that floods are good for salmonids if they allow them to reach otherwise inaccessible spawning grounds. Here, however, the trouble comes if the redds (egg nests) are exposed to environmental stress when the water recedes: the instinct to seek suitable gravels high in the river channel could prove a poor adaptation if the channel then dries in spring, causing the eggs to die – a type of evolutionary trap. However, if the flood duration were sufficient to maintain suitable redd conditions, then these otherwise inaccessible spawning grounds could prove to be highly productive.\nOne of the most sensitive salmonid life stages is the egg. Salmonids lay their eggs in gravels, in part, to protect them from adverse environmental conditions, such as high flows. Under flood conditions, high velocity water can wash eggs out of redds causing them to die. This is particularly true for grayling that lay their eggs shallow in redds. This is also more likely in rivers with hard bedrock that tend to react quickly and fiercely to rainfall events. On the other hand, a comparatively low magnitude or long duration flood could wash sediments out of redds that would otherwise asphyxiate the developing eggs, thereby improving egg survival.\nIf eggs survive the floods, then they emerge as fry. Fry tend to stay in the shallow and protected waters at the edges of river channels to avoid predators and because they are not strong swimmers. High flows during, or shortly after emergence has been associated with high fry mortality – they too can be washed out of the river and die, whether down the river channel into saline water or over the edge of the channel onto adjoining land. However, fry swimming ability increases quickly with size and so the timing of the flood is critical to fry survival.\nSummer floods are perhaps less common and so their effects are even less well understood. One factor thought to affect the growth and development of juvenile salmonids is the number of other juvenile salmonids, through density dependent growth: as the number of juvenile salmon increases, so the area available for each individual to find food and grow decreases. Assuming density dependent growth, then a summer flood that increases the wetted area of the river might convey increased growth rates and better conditioned juveniles with a higher chance of survival. With their improved swimming ability compared to fry, juveniles are less susceptible to washout during floods.\nOnce beyond the juvenile stage, the effect of flow on salmon and sea trout becomes important for migration timing. Flows facilitate the downstream migration of smolts. High flows and associated turbidity could increase the rate at which smolts migrate out of the river in murky water, both of which will help to lessen the risk of predation.\nWhen it comes to re-entering the river as a spawning adult, then flow again appears to be a trigger, although the exact nature of the relationship between flow and migration timing has proven elusive:\n\nIt is not clear whether they move on or after floods (or freshets) or, indeed, whether they are even present in the river mouth at the time of floods\nThere is high inter-river variability; for example, flow might be less influential on chalk streams with more stable flow conditions relative to granite rivers.\n\nOnce in the river, salmon migration occurs in three phases:\n\nThe upstream migration phase\nThe spawning ground searching phase\nThe spawning site holding phase\n\nOf these, phase 1 is the most affected by flow. The speed of upstream migration seems to increase with increasing flow, although in floods the salmon may stop moving and be forced to take refuge. Again, the exact timing of these different phases is likely to be associated with floods or freshets, but the precise nature of that association remains elusive. It seems clear that flooding can affect salmonids in many ways and developing an understanding of the interactions and trade-offs will require the use of high-quality, long-term (or well-replicated) datasets that are difficult to collect in aquatic systems. Fish cannot simply be counted like birds because they can’t be seen without additional apparatus. This is further complicated because water is corrosive, conductive and carries debris and huge energy, particularly during flood conditions.\nAt East Stoke, on the River Frome in Dorset, the GWCT has been monitoring adult, smolt and parr salmon for the last 44, 21 and 16 years, respectively. To do this, we use a range of monitoring equipment, including a resistivity counter, a 24-hour video surveillance camera and a system of electronic tag-reading devices (approximately 10,000 juvenile salmon are marked with electronic tags each year). For the large majority of the time, this equipment does an excellent job of detecting migrating salmon. Rarely, however, when flow conditions are extraordinary, such as under major flood conditions, the efficiency of the equipment is compromised.\nTake the spring and summer floods of 2012 as an example. Daily flows from the end of April to the end of September were unprecedented among records dating back to 1992 (Figure 1). These extraordinary flows forced us to lift our electronic readers on the East Stoke salmon counter (Figures 2) and reduced our resistivity and video counter efficiency. Consequently, we reported a minimum adult salmon count in 2012. Also, because the floods began at the end of April, they impinged on our ability to quantify the migration of smolts from the river, which usually ends in late April or early May.\n\n\n\nDaily flows for 2012 (black line) compared to flows for years 1992 to 2011 (grey lines).\n\n\n\n\n\nThe East Stoke fish counter under normal flow conditions in 2010 and under flood in 2012.\n\n\nDespite sometimes treacherous conditions, there are only three other years when we have reported a minimum adult salmon count. Figure 3 shows that we have recorded three years with extraordinary high median monthly flows (1994, 2000 and 2012). In two of these years – 2000 and 2012 – we reported a minimum adult salmon count. In contrast, we were able to produce an accurate count in 1994, despite the extraordinary flow conditions. This highlights two points:\n\nThat we can produce counts under flood conditions but they might be less accurate\nThere might be times when our counting system malfunctions for reasons other than flow, in which we can produce only a minimum count, such as occurred in 1989 (Figure 3).\n\n\n\n\nBoxplots showing monthly flows for each year and highlighting those years for which (1) we were unable to produce an accurate adult salmon count (red box), and (2) the river was in flood (blue fill). The dashed line is the mean monthly flow excluding flood years.\n\n\nWith growing evidence that we’re being propelled ever faster towards the wetter and warmer winters predicted by climate change scientists a decade ago, we’ll have to find ways to cope with flooding and the effects it will have on our salmon and grayling monitoring programmes. We need also, however, to be mindful of the effects on our salmonid populations and our ability to monitor them during drought conditions, which are forecast to become more frequent and intense.\nOne of the ways in which we are attempting to do this is statistically. Within the MorFish project, we designed a statistical framework that allows us to use information from periods when the salmon counting system is operational to predict what the salmon count was likely during periods when the system is malfunctioning. In addition, the analysis uses observations to learn about the system efficiency under variable flow conditions and use that information to improve estimates during periods of low efficiency. Finally, because floods are associated with other environmental factors, e.g., increased turbidity, we aim to extend the framework to investigate whether these additional factors will change our salmon count estimates.\nIt is hoped that these analyses will deliver useful information on the effects of extreme flows (droughts and floods) on the health of salmon – and more generally, fish – population dynamics. What we then do to manage flow regimes are decisions that will need to be made at a local scale, since flow regimes are tied into many local factors, including geology, river-specific hydroecology and hydromorphology and – of course – abstraction regimes. Certainly, the answer will lie in maintaining the ecological integrity of the river. One step to doing this will be to understand and encourage its natural hydrological variability, both seasonally and interannually, an approach known as the “natural flow paradigm”.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Salmonids in the Floods},\n  date = {2016-03-10},\n  url = {https://stephendavidgregory.github.io/posts/salmonids-in-floods},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Salmonids in the Floods.” March\n10, 2016. https://stephendavidgregory.github.io/posts/salmonids-in-floods."
  },
  {
    "objectID": "posts/smolt-run-2017/index.html",
    "href": "posts/smolt-run-2017/index.html",
    "title": "Get ready… set… go! Frome salmon smolt run 2017",
    "section": "",
    "text": "Every year, the GWCT monitors migrating juvenile Atlantic salmon (Salmo salar L.) smolts leaving the river Frome, Dorset, on their way to their marine feeding grounds, where they go to grow and mature before returning to the Frome to reproduce.\nFor more info, read the GWCT webpage.\nHere, I want to explain why we collect data on salmon smolts, how we collect it, and briefly outline how we process the data."
  },
  {
    "objectID": "posts/smolt-run-2017/index.html#estimated-number-of-marked-smolts",
    "href": "posts/smolt-run-2017/index.html#estimated-number-of-marked-smolts",
    "title": "Get ready… set… go! Frome salmon smolt run 2017",
    "section": "Estimated number of marked smolts",
    "text": "Estimated number of marked smolts\nGWCT have installed PIT tag readers, hereafter known as antennae, at several strategic locations throughout the river Frome, Dorset.\n\n\n\nSchematic showing the locations of PIT tag antennae at East Stoke, Dorset.\n\n\nAs PIT-tagged salmon smolts migrate past an antenna, the antenna transmits power to the PIT tag, which then transmits its unique code to the antenna, informing us that the smolt has moved past that antenna.\nIf the antennae were 100% efficient at detecting migrating PIT-tagged salmon smolts, then the total number of unique codes recorded by an antenna would be the estimate of smolts moving past that antenna.\nThe antennae are not, however, 100% efficient; i.e., detection is imperfect (for a similar problem, see my post about acoustic tagging sea trout smolts).\nTo overcome this, we can use a capture-mark-recapture experiment in which one antennae is used to calculate the “efficiency” of the other.\n\n\n\nThe first PIT tag antenna installed in an artificial channel the Fluvarium.\n\n\n\n\n\nThe second antenna installed on a weir downstream of the Fluvarium. This antenna is used to calculate the efficiency of the first antenna.\n\n\nIn our CMR experiment, antennae are used to “sight” and “resight” individuals. In classical CMR lingo, the number of unique codes seen on the “first” antenna in the Fluvarium, i.e., the location where PIT-tagged salmon smolts are first sighted, is \\(M\\) and the number of unique codes seen on the “second” Flatbed antenna on the weir, i.e., the location where PIT-tagged salmon smolts are resighted, is \\(C\\). The Lincoln-Petersen (Lincoln, 1930; Petersen, 1896) method for estimating population size \\(\\hat{N}\\) is then\n\\[\\hat{N} = MC / R\\]\nwhere \\(R\\) is the number of PIT-tagged resightings in the second capture.\nUsing these simple equations, we are able to estimate the population of PIT-tag marked salmon smolts."
  },
  {
    "objectID": "posts/smolt-run-2017/index.html#ratio-of-markedunmarked-salmon-smolts",
    "href": "posts/smolt-run-2017/index.html#ratio-of-markedunmarked-salmon-smolts",
    "title": "Get ready… set… go! Frome salmon smolt run 2017",
    "section": "Ratio of marked:unmarked salmon smolts",
    "text": "Ratio of marked:unmarked salmon smolts\nOur CMR experiment gives us an estimate of the population of PIT-tag marked salmon smolts. This, however, is only a small part of the popultion: we PIT tag only a fraction of the population of salmon (and trout) juveniles in August and September each year (see the GWCT webpage for more details).\nBut, we are interested in estimating the total salmon smolt population size, i.e., the PIT-tag marked and the unmarked populations combined.\nTo do this, we can divide the estimate of PIT-tag marked salmon smolt population by the proportion of salmon smolts that are PIT-tag marked by\n\\[\\hat{N}_{total} = \\hat{N}_{marked} / Q\\]\n\\[Q = n_{marked} / n_{total}\\]\nwhere \\(Q\\) is the proportion of salmon smolts that are PIT-tag marked and \\(n\\) is a sample of smolts.\nBut how do we get a sample of smolts from which to measure \\(n_{marked}\\) and \\(n_{total}\\)?\nWe use a Rotary Screw Trap (RST) to capture a sample of migrating salmon smolts.\n\n\n\nOur Rotary Screw Trap (RST) on the river Frome, Dorset.\n\n\nRunning the RST is not an easy job. When conceiving the sampling protocol, we wanted to ensure that:\n\nOur effect on these sensitive migrating salmon smolts was minimised;\nWe took an adequate sample of the smolts throughout the duration of their migration; and\nWe sampled smolts migrating at different times during the dirunal cycle.\n\nTo meet these (and other) requirements, we decided to run the RST during the day and during the night for the entire duration of the smolt migration and to check the RST every 30 minutes. By checking the RST every 30 minutes, it was hoped that we would minimise our effect on migrating smolts, returning them back to the river no more than 1 hour after they first entered the RST, i.e., night-time migrating smolts could continue their migration under the cover of night and vice versa.\nOver the course of the smolt migration, we capture \\(n_{total}\\) smolts and record \\(n_{marked}\\) marked smolts. Using these numbers, we can calculate \\(Q\\) and then estimate the total smolt population size \\(N\\).\nWhile we examine the smolts for PIT tags, we also collect some biometric data on their condition. For this, they make a visit to the “smolt table”.\n\n\n\nOur smolt table where we take biometric measurements of all the smolts that we capture in the RST.\n\n\nOur procedures are designed to minimise disturbance to the smolts and they are all returned to the river carefully and quickly to continue their epic journey to their feeding grounds.\n\n\n\nOur release point on the river Frome, where processed smolts return to their migration (usually, much) less than 1 hour since they were captured in the RST."
  },
  {
    "objectID": "posts/tamar-visit/index.html",
    "href": "posts/tamar-visit/index.html",
    "title": "Tamar visit",
    "section": "",
    "text": "I, together with my GWCT colleagues, was lucky enough to visit the river Tamar, on the Devon / Cornwall border in Southwest England.\nThe Tamar is an Atlantic salmon (Salmo salar) “Index River”, i.e., a river on which the salmon population is monitored and used as an indication of the health of the total Atlantic salmon population in Europe. As part of the monitoring programme, the Environment Agency (EA) run a adult and juvenile salmon traps each year and used the records to estimate the adult and juvenile population size (using Capture-Mark-Recapture estimates).\nBelow are a few photos that I took during my visit.\nA huge thanks to the Environment Agency for allowing us to visit :)\n     \n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2016,\n  author = {D. Gregory, Stephen},\n  title = {Tamar Visit},\n  date = {2016-11-22},\n  url = {https://stephendavidgregory.github.io/posts/tamar-visit},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2016. “Tamar Visit.” November 22,\n2016. https://stephendavidgregory.github.io/posts/tamar-visit."
  },
  {
    "objectID": "posts/ukgrid-to-latlong/index.html",
    "href": "posts/ukgrid-to-latlong/index.html",
    "title": "Coordinate conversions in R",
    "section": "",
    "text": "Here, I present a short procedure for converting British National Grid coordinates into latitude and longitude coordinates.\n\nBackground\nMany times, I have received data using the British National Grid coordinate system (OSGB36 datum, EPSG code: 27700; also known as the Ordnance Survey National Grid). It is excellent that these spatial data are collected and I implore people to continue to record where observations are collected.\nMost spatial analyses are, however, written with more generic coordinate systems in mind. One such coordinate system is the World Geodetic System (WGS84 datum, EPSG code: 4326), which is the reference coordinate system for Global Positioning System.\nI often want to use the spatial data in analyses, or at least to verify visually that they look correct, and this is easiest when the spatial data is in WGS84. So, I convert the coordinates.\nBelow is a generic version of the script that I use.\n\n\nR script\n\n# read in csv data; first column is assumed to be Easting and second Northing\ndat &lt;- read.csv('BNGpoints.csv')\n\n# rename columns\ncolnames(dat)[c(1, 2)] &lt;- c('Easting', 'Northing')\n\n# libraries\nrequire(rgdal) # for spTransform\nrequire(stringr)\n\n## shortcuts\nukgrid &lt;- \"+init=epsg:27700\"\nlatlong &lt;- \"+init=epsg:4326\"\n\n## Create coordinates variable\ncoords &lt;- cbind(Easting = as.numeric(as.character(dat$Easting)),\n                Northing = as.numeric(as.character(dat$Northing)))\n\n## Create the SpatialPointsDataFrame\ndat_SP &lt;- SpatialPointsDataFrame(coords,\n                                 data = dat,\n                                 proj4string = CRS(\"+init=epsg:27700\"))\n\n## Convert\ndat_SP_LL &lt;- spTransform(dat_SP, CRS(latlong))\n\n# replace Lat, Long\ndat_SP_LL@data$Long &lt;- coordinates(dat_SP_LL)[, 1]\ndat_SP_LL@data$Lat &lt;- coordinates(dat_SP_LL)[, 2]\n\n# optionally write out as shapefile\nwriteOGR(obj = dat_SP_LL, dsn = '.', layer = 'BNGpoints', driver = 'ESRI Shapefile')\nHere is an example of salmon redd locations converted from BNG to WGS84 and plotted on a shapefile of the river Frome, Dorset, UK.\n\n\n\nAtlantic salmon redds on the River Frome\n\n\nI hope this is useful to someone.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2017,\n  author = {D. Gregory, Stephen},\n  title = {Coordinate Conversions in {R}},\n  date = {2017-01-07},\n  url = {https://stephendavidgregory.github.io/posts/ukgrid-to-latlong},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2017. “Coordinate Conversions in R.”\nJanuary 7, 2017. https://stephendavidgregory.github.io/posts/ukgrid-to-latlong."
  },
  {
    "objectID": "themes.html",
    "href": "themes.html",
    "title": "Themes",
    "section": "",
    "text": "My research can be classified into the following themes:"
  },
  {
    "objectID": "themes.html#environmental-change",
    "href": "themes.html#environmental-change",
    "title": "Themes",
    "section": "Environmental change",
    "text": "Environmental change\n\n\nI believe that environmental change is among the greatest threats to the persistence of wild animal and plant populations. As a conservation biologist, environmental change, including climate and land-use change and their interaction, is a theme that runs through much of my work, from Malaysian orangutan to Atlantic salmon. Of particular note, is the role of environmental change on different life-stages of individuals from the same population(s), which can be seen in my work on Atlantic salmon smolt sizes and the River Wylye Grayling population dynamics.\n\n\n\n\n\n\nEnvironmental change\n\n\n\n\n\nYou can find my blogs in this theme, including summaries of papers, here."
  },
  {
    "objectID": "themes.html#tagging-and-telemetry",
    "href": "themes.html#tagging-and-telemetry",
    "title": "Themes",
    "section": "Tagging and telemetry",
    "text": "Tagging and telemetry\n\n\nTagging and telemetry and associated capture-mark-recapture (CMR) studies are an increasing important part of my everyday work, whether it’s estimating the survival of salmon smolts returning from sea, or loss rates of acoustically tagged sea trout smolts. I tend to specify my CMR analyses in Bayseian languages, including stan, JAGS or greta, among others. I find these analyses intuitive and so am able to specify statistical models that are well-adapted to the data collection protocol and data, including using state-space models that separate the ecological and observation processes.\n\n\n\n\n\n\nTagging and telemetry\n\n\n\n\n\nYou can find my blogs in this theme, including summaries of papers, here."
  },
  {
    "objectID": "themes.html#allee-effects",
    "href": "themes.html#allee-effects",
    "title": "Themes",
    "section": "Allee effects",
    "text": "Allee effects\n\n\nAllee effects were the subject of my PhD and I have published a few papers on the topic. I keep abreast of the topic through reading and reviewing and it is always on my mind as to how I can revisit the topic. I have plans (although not particularly well-formed) to study Allee effects in several contexts, from empirical studies of shoaling in migratory fish to simulation studies of interactions among (sub-)group- and population-level component and demographic Allee effects.\n\n\n\n\n\n\nAllee effects\n\n\n\n\n\nYou can find my blogs in this theme, including summaries of papers, here."
  },
  {
    "objectID": "themes.html#invasive-species",
    "href": "themes.html#invasive-species",
    "title": "Themes",
    "section": "Invasive species",
    "text": "Invasive species\n\n\nInvasive species threaten populations of native species throughout the world, but particularly on islands. I have studied invasive species at various points in my career, including how invasive species affect native populations, such as my MRes study on invasive black rats in the Galapagos Islands, and how those invasive species can be managed, such as my works on prioritising invasive species eradication strategies on islands. It is a topic that I hope to keep involved with.\n\n\n\n\n\n\nInvasive species\n\n\n\n\n\nYou can find my blogs in this theme, including summaries of papers, here."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Warning: NAs introduced by coercion"
  },
  {
    "objectID": "posts/coocurrence/index.html",
    "href": "posts/coocurrence/index.html",
    "title": "Understanding co-occurrence",
    "section": "",
    "text": "Here, I simulate a species co-occurrence problem using only base R and R package data.table.\n\nSimulation\n\n# libraries\nlibrary(data.table)\n\nObserved co-occurrence can be compared to the expected co-occurrence where the latter is the product of the two species’ probability of occurrence multiplied by the number of sampling sites: \\(E(N_{1,2}) = P(1) \\times P(2) \\times N\\).\n\n# number of sampling sites (N)\nN &lt;- 1000\n\n# probability of encountering species 1\nP1 &lt;- 0.3\n\n# probability of encountering species 2\nP2 &lt;- 0.1\n\n# expected number of sites where they co-occur\nN_12 &lt;- P1 * P2 * N\n\n# print result\nprint(N_12)\n\n[1] 30\n\n\nIt can be easier to understand this using a simulation:\n\n# repeat simulation n times\nn &lt;- 10000\n\n# sites & simulation number\ns &lt;- data.table(\"site\" = rep(1:N, n),\n                \"simulation\" = rep(1:n, each = N))\n\n# add species 1 occurrences\ns[, `species 1` := sample(c(0, 1), size = N * n, replace = TRUE, prob = c(1 - P1, P1))]\n\n# add species 2 occurrences\ns[, `species 2` := sample(c(0, 1), size = N * n, replace = TRUE, prob = c(1 - P2, P2))]\n\n# add coocurrence\ns[, `expct cooccur` := as.numeric((`species 1` + `species 2`) == 2)]\nprint(s)\n\n          site simulation species 1 species 2 expct cooccur\n       1:    1          1         0         0             0\n       2:    2          1         1         0             0\n       3:    3          1         0         0             0\n       4:    4          1         0         1             0\n       5:    5          1         0         0             0\n      ---                                                  \n 9999996:  996      10000         1         0             0\n 9999997:  997      10000         1         0             0\n 9999998:  998      10000         1         0             0\n 9999999:  999      10000         0         0             0\n10000000: 1000      10000         0         0             0\n\n# number of times the cooccur on average, i.e., expectation\nexpct &lt;- s[, .(c = sum(`expct cooccur`)), \n           by = simulation][, .(Expectation = mean(c))]\n\n# print result\nprint(expct)\n\n   Expectation\n1:     29.9796\n\n\nLet us assume that we observe these species co-occurring at 35 sites. How likely is it that this is more often than expected by chance?\n\n# number cooccurrences\nl &lt;- 35\n\n# observed coocurrences\no &lt;- do.call(\"rbind\",\n             list(matrix(rep(c(1, 1), l), \n                         ncol = 2, byrow = TRUE),\n                  matrix(rep(c(1, 0), (N * P1) - l), \n                         ncol = 2, byrow = TRUE),\n                  matrix(rep(c(0, 1), (N * P2) - l), \n                         ncol = 2, byrow = TRUE),\n                  matrix(rep(c(0, 0), N - (l + ((N * P1) - l) + ((N * P2) - l))), \n                         ncol = 2, byrow = TRUE)))\n\n# add to simulations\ns[, `obsrv cooccur` := rep(as.numeric(rowSums(o) == 2), n)]\n\n# how often is expct &lt;= obsrv?\np &lt;- s[, sum(`obsrv cooccur`) &lt;= sum(`expct cooccur`), \n       by = simulation][, .(n = sum(V1), p = mean(V1))]\n\n# print result\nprint(p)\n\n      n      p\n1: 1941 0.1941\n\n\nWe can show this on a histogram of the number of co-occurrences in the simulations with the actual number of co-occurrences shown in red.\n\n# plot histogram\nfoo &lt;- s[, .(c = sum(`expct cooccur`)), \n       by = simulation]$c\nhist(foo,\n     main = \"Histogram of simulated number of co-occurrences\",\n     xlab = \"Number of co-occurrences\",\n     xlim = c(min(foo), max(c(max(foo, l)) + 10)))\nabline(v = l, col = \"red\", lwd = 3)\nmtext(paste0(\"p = \", p$p), \n      side = 3, line = 0, at = l, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nWhat if!???\nThanks to Robert Thorpe for an interesting question:\n“If P1 and P2 both decreased by 50%, e.g., if there was an unproductive year in the ecosystem, the co-occurrence expected would go down by 75% other things being equal?”\n\n# number of sampling sites (N) - same\nN &lt;- 1000\n\n# probability of encountering species 1 - 50% lower\nP1 &lt;- 0.3 * 0.5\n\n# probability of encountering species 2 - 50% lower\nP2 &lt;- 0.1 * 0.5\n\n# expected number of sites where they co-occur\nN_12 &lt;- P1 * P2 * N\n\n# print result\nprint(N_12)\n\n[1] 7.5\n\n\nwhich should be the same as 75% fewer coocurrences:\n\n# check\n30 * (1 - 0.75)\n\n[1] 7.5\n\n\n\n\nReading\n\nThis can be done more efficiently using a combinatorial approach: see https://thatdarndata.com/understanding-species-co-occurrence/\nR package cooccur uses a probabilistic approximation to the combinatorial approach and is still faster\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2023,\n  author = {D. Gregory, Stephen},\n  title = {Understanding Co-Occurrence},\n  date = {2023-02-09},\n  url = {https://stephendavidgregory.github.io/posts/coocurrence},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2023. “Understanding Co-Occurrence.”\nFebruary 9, 2023. https://stephendavidgregory.github.io/posts/coocurrence."
  },
  {
    "objectID": "posts/nrw-recruitment-crash/index.html",
    "href": "posts/nrw-recruitment-crash/index.html",
    "title": "New paper: Environmental conditions modify density‐dependent salmonid recruitment: Insights into the 2016 recruitment crash in Wales",
    "section": "",
    "text": "Our paper “Environmental conditions modify density‐dependent salmonid recruitment: Insights into the 2016 recruitment crash in Wales” is out today in Freshwater Biology!\nGregory, S.D., Bewes, V.E., Davey, A.J.H., Roberts, D.E., Gough, P., Davidson, I.C. (2020) Environmental conditions modify density-dependent salmonid recruitment: Insights into the 2016 recruitment crash in Wales. Freshwater Biology, 65: 2135–2153. https://doi.org/10.1111/fwb.13609\nIn it, we explore whether the apparent juvenile salmon recruitment crash of 2015 was indeed “a crash”, i.e., unprecedented in recent times, and what might have caused it. We conclude that it was a recruitment crash for age 0+ juvenile salmon - but not trout - and that it seemed to be related to high over-winter temperatures, which likely interrupted spawning behaviour or affected egg survival - see figure below.\n\n\n\n\n\n\nAbstract\n\nUnderstanding the effects of density-dependent and density-independent factors on recruitment is often inhibited by difficulties quantifying their relative contributions in highly variable recruitment data. Use of data-driven statistical methods with data that include one or more extreme recruitment events could help overcome these difficulties.\nJuvenile Atlantic salmon and trout abundances in Wales have declined over the last 2 decades, and 2016 was a notably poor recruitment year in rivers around southern Europe, including England and Wales. The 2016 recruitment crash coincided with extreme winter weather conditions, leading to speculation that unusually warm temperatures and high flows adversely affect salmonid recruitment and caused the 2016 crash, although this remains untested.\nWe developed data-driven statistical models to: (1) describe juvenile salmonid recruitment from density-dependent and density-independent factors; and (2) assess whether the density-independent factors probably contributed to the 2016 salmon recruitment crash. We compiled salmon and trout young-of-year juvenile abundances from electrofishing surveys, egg deposition estimates and river flow and air temperature data from 2001–2017 for seven Welsh rivers, broadly representative of rivers around Wales. We used river flow and air temperature data to derive ecologically and behaviourally meaningful density-independent explanatory variables.\nSalmonid recruitment in Wales was best described using density-dependent and density-independent factors, especially for salmon: after accounting for a concave relationship with egg deposition, salmon juvenile abundance was reduced under (1) warmer spawning temperatures that might inhibit spawning, and (2) higher flood frequencies during pre-emergence and emergence that might washout eggs or alevins. Results were less clear for trout, perhaps because they are behaviourally more plastic.\nOur findings provide empirical support for general and predictable effects of temperature and flow during spawning and emergence on salmonid—especially salmon—recruitment in Wales. Furthermore, we suggest that the 2016 salmon recruitment crash was caused—in part—by particularly inclement spawning and emergence conditions, which could be more common under future climate change. Our findings suggest that future salmonid stock assessment models could include the effects of density-independent variables on recruitment to improve their predictive power.\n\nA pdf reprint is available here: Gregory2020.pdf\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2020,\n  author = {D. Gregory, Stephen},\n  title = {New Paper: {Environmental} Conditions Modify\n    Density‐dependent Salmonid Recruitment: {Insights} into the 2016\n    Recruitment Crash in {Wales}},\n  date = {2020-09-04},\n  url = {https://stephendavidgregory.github.io/posts/nrw-recruitment-crash},\n  langid = {en},\n  abstract = {\\$\\^{}\\{0\\}\\$ *Formerly* Salmon \\& Trout Research Centre,\n    Game and Wildlife Conservation Trust, East Stoke, Wareham, Dorset\n    BH20 6BB, UK \\$\\^{}\\{1\\}\\$ The Centre for Environment, Fisheries and\n    Aquaculture Science, Barrack Road, Weymouth, Dorset DT4 8UB, UK\n    \\$\\^{}\\{2\\}\\$ Department of Life and Environmental Sciences, Faculty\n    of Science and Technology, Bournemouth University, Poole BH12 5BB,\n    UK}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2020. “New Paper: Environmental Conditions\nModify Density‐dependent Salmonid Recruitment: Insights into the 2016\nRecruitment Crash in Wales.” September 4, 2020. https://stephendavidgregory.github.io/posts/nrw-recruitment-crash."
  },
  {
    "objectID": "posts/cjs-tmb-is-fast/index.html",
    "href": "posts/cjs-tmb-is-fast/index.html",
    "title": "CJS is fast in TMB",
    "section": "",
    "text": "A popular way to estimate life-history rates, such as survival, reproduction and movement rates, is using Cormack Jolly Seber (CJS) models cast in state-space equations. These models vary in complexity, but generally build on two fairly simple equations representing state and observation processes:\n\\[\nz_{i,t} \\sim \\mbox{Bernoulli}(z_{i,t - 1}, \\varphi)\n\\]\n\\[\ny_{i,t} \\sim \\mbox{Bernoulli}(z_{i,t}, p)\n\\]\nThe first equation is known as the state process and allows for the estimation of the transition rate, here survival \\(\\varphi\\), from the state \\(z\\) of individual \\(i\\) and time \\(t\\) from their state at time \\(t-1\\). The second equation is known as the observation process and allows for the estimation of the recapture or resighting rate \\(p\\) of the individual in state \\(z\\) at time \\(t\\).\nI’ve used CJS models in a number of capacities (see ), and for each I’ve written my own model in OpenBUGS, JAGS, Stan or nimble, and used Bayesian inference. However, Bayesian inference relies on sampling the parameter space using Monte-Carlo Markov Chains (MCMC), which can be time-consuming (despite ever-efficient algorithms and computation).\nRecently, I attended fantastic Template Model Builder training given by Anders Nielsen. It is promoted as a fast and flexible language for inference, whether frequentist or Bayesian.\nI thought that it might be nice to write a simple CJS model in TMB, following the excellent examples by Olivier Gimenez, and in native R too.\nHere is my try…"
  },
  {
    "objectID": "posts/cjs-tmb-is-fast/index.html#data-simulation",
    "href": "posts/cjs-tmb-is-fast/index.html#data-simulation",
    "title": "CJS is fast in TMB",
    "section": "Data simulation",
    "text": "Data simulation\nWe first build a function to simulate data from a multi-event (juvenile and adult) capture-recapture (CJS) model:\n\nsimul &lt;- function(\n    n_individuals = 1000,\n    n_occasions = 3, # release, first sighting, second sighting\n    n.states = 3, # juvenile, adult, dead\n    phi = 0.7,\n    psi = 0.8,\n    p = 0.3) { \n  \n  # state matrix\n  phi_state &lt;- matrix(c(\n    \n    0, phi, 1 - phi,\n    0, psi, 1 - psi,\n    0, 0, 1\n    \n  ), nrow = n.states, byrow = TRUE)\n  \n  # observation matrix \n  phi_obs &lt;- matrix(c(\n    \n    0, 0, 1,\n    0, p, 1 - p,\n    0, 0, 1\n    \n  ), nrow = n.states, byrow = TRUE)\n  \n  # empty capture history list of matrices\n  ch &lt;- ch_true &lt;- matrix(NA, \n                          nrow = n_individuals, \n                          ncol = n_occasions)\n  \n  # always seen at release\n  ch[, 1] &lt;- ch_true[, 1] &lt;- 1\n  \n  # fill CH\n  for (i in 1:n_individuals) {\n    for (o in 2:n_occasions) {\n      # state transition\n      state_p &lt;- rmultinom(n = 1, \n                           size = 1, \n                           prob = phi_state[ch_true[i, o - 1], ])\n      ch_true[i, o] &lt;- which(state_p == 1)\n      # observation transition\n      event_p &lt;- rmultinom(n = 1, \n                           size = 1, \n                           prob = phi_obs[ch_true[i, o], ])\n      ch[i, o] &lt;- which(event_p == 1)\n    } # o\n  } # i\n  \n  # return\n  return(list(\"ch\" = ch, \"ch.true\" = ch_true))\n  \n}"
  },
  {
    "objectID": "posts/cjs-tmb-is-fast/index.html#likelihood-function-in-native-r",
    "href": "posts/cjs-tmb-is-fast/index.html#likelihood-function-in-native-r",
    "title": "CJS is fast in TMB",
    "section": "Likelihood function in native R",
    "text": "Likelihood function in native R\nFirst, we need a small function to protect the log from “exploding” (thanks, Olivier Gimenez):\n\nlogprot &lt;- function(v) {\n  eps &lt;- 2.2204e-016\n  u &lt;- log(eps) * (1 + vector(length = length(v)))\n  index &lt;- (v &gt; eps)\n  u[index] &lt;- log(v[index])\n  return(u)\n}\n\nThen the likelihood (take the time to compare this function to the data simulation function - they’re similar!):\n\ncjs &lt;- function(b, ch) { \n  \n  # R version of the TMB function below - not used here \n  multvecmat &lt;- function(A, B) {\n    nrowb &lt;- nrow(B)\n    ncolb &lt;- ncol(B)\n    C &lt;- vector(\"numeric\", length = ncolb)\n    for (i in 1:ncolb) {\n      C[i] &lt;- 0\n      for (k in 1:nrowb) {\n        C[i] &lt;- C[i] + A[k] * B[k, i]\n      }\n    }\n    return(C)\n  }\n  \n  # parameters\n  n_individuals &lt;- nrow(ch)\n  n_occasions &lt;- ncol(ch)\n  \n  # transforms\n  phi &lt;- plogis(b[1])\n  psi &lt;- plogis(b[2])\n  p &lt;- plogis(b[3])\n  \n  # state matrix\n  A &lt;- matrix(c(\n    0, phi, 1 - phi,\n    0, psi, 1 - psi,\n    0, 0, 1\n  ), nrow = 3, byrow = TRUE)\n  \n  # observation matrix\n  B &lt;- matrix(c(\n    0, 0, 1,\n    0, p, 1 - p,\n    0, 0, 1\n  ), nrow = 3, byrow = TRUE)\n  \n  # log likelihood\n  l &lt;- 0\n  for (i in 1:n_individuals) {\n    foo &lt;- c(1, 0, 0)\n    for (j in 2:n_occasions) {\n      foo &lt;- (foo %*% A) * B[, ch[i, j]]\n    }\n    l &lt;- l + logprot(sum(foo))\n  }\n  \n  # negative loglikelihood\n  l &lt;- -l\n  \n  # return\n  return(l)\n  \n}"
  },
  {
    "objectID": "posts/cjs-tmb-is-fast/index.html#likelihood-function-in-tmb",
    "href": "posts/cjs-tmb-is-fast/index.html#likelihood-function-in-tmb",
    "title": "CJS is fast in TMB",
    "section": "Likelihood function in TMB",
    "text": "Likelihood function in TMB\nNow, let me try and write the same thing in TMB:\n\ntmb_model &lt;- \"\n\n// capture-recapture model\n#include &lt;TMB.hpp&gt;\n\n/* implement the vector - matrix product */\ntemplate&lt;class Type&gt;\nvector&lt;Type&gt; multvecmat(vector&lt;Type&gt; A, matrix&lt;Type&gt; B) {\n  int nrowb = B.rows();\n  int ncolb = B.cols(); \n  vector&lt;Type&gt; C(ncolb);\n  for (int i = 0; i &lt; ncolb; i++) {\n    C(i) = Type(0);\n    for (int k = 0; k &lt; nrowb; k++) {\n      C(i) += A(k) * B(k, i);\n    }\n  }\n  return C;\n}\n\ntemplate&lt;class Type&gt;\nType objective_function&lt;Type&gt;::operator() () {\n\n  // data\n  DATA_IMATRIX(ch);\n  int n_individuals = ch.rows();  \n  int n_occasions = ch.cols();\n  // DATA_VECTOR(fii);\n  \n  // parameters\n  PARAMETER_VECTOR(b);\n  \n  // transformations\n  int npar = b.size();\n  vector&lt;Type&gt; par(npar);\n  for (int i = 0; i &lt; npar; i++) {\n    par(i) = Type(1.0) / (Type(1.0) + exp(-b(i)));\n  }\n  Type phi = par(0);\n  Type psi = par(1);\n  Type p = par(2);\n  \n  // observation matrix\n  matrix&lt;Type&gt; A(3, 3);\n  A(0, 0) = Type(0.0);\n  A(0, 1) = phi;\n  A(0, 2) = Type(1.0) - phi;\n  A(1, 0) = Type(0.0);\n  A(1, 1) = psi;\n  A(1, 2) = Type(1.0) - psi;\n  A(2, 0) = Type(0.0);\n  A(2, 1) = Type(0.0);\n  A(2, 2) = Type(1.0);\n  \n  // observation matrix\n  matrix&lt;Type&gt; B(3, 3);\n  B(0, 0) = Type(0.0);\n  B(0, 1) = Type(0.0);\n  B(0, 2) = Type(1.0);\n  B(1, 0) = Type(0.0);\n  B(1, 1) = p;\n  B(1, 2) = Type(1.0) - p;\n  B(2, 0) = Type(0.0);\n  B(2, 1) = Type(0.0);\n  B(2, 2) = Type(1.0);\n  \n  // likelihood\n  Type ll;\n  Type nll;\n  for (int i = 0; i &lt; n_individuals; i++) {\n    vector&lt;Type&gt; foo(3);\n    foo(0) = Type(1.0);\n    foo(1) = Type(0.0);\n    foo(2) = Type(0.0);\n    for (int j = 1; j &lt; n_occasions; j++) {\n      foo = multvecmat(foo, A) * vector&lt;Type&gt; (B.col(ch(i, j)));\n    }\n    ll += log(sum(foo));\n  }\n  \n  // negative loglikelihood\n  nll = -ll;\n  \n  // return\n  return nll;\n  \n}\"\n\nThen, we write the code to disk, compile and load it, which requires the R package TMB:\n\nwrite(tmb_model, file = \"cjs_tmb.cpp\")\n\nlibrary(TMB)\nif (!file.exists(\"cjs_tmb.dll\")) compile(\"cjs_tmb.cpp\")\ndyn.load(dynlib(\"cjs_tmb\"))"
  },
  {
    "objectID": "posts/cjs-tmb-is-fast/index.html#estimation",
    "href": "posts/cjs-tmb-is-fast/index.html#estimation",
    "title": "CJS is fast in TMB",
    "section": "Estimation",
    "text": "Estimation\nWith that done, let’s see how estimates from the two approaches compare and their speed:\n\n# data simulation\nraw_data &lt;- simul(\n  n_individuals = 10000,\n  phi = 0.6,\n  psi = 0.9,\n  p = 0.2\n)\n\n# initial parameter values\nb &lt;- runif(3, -1, 1)\n\n# extract the data\nch &lt;- raw_data$ch\n\n# R - using optim\ntic &lt;- Sys.time()\nfaa &lt;- optim(par = b, \n             fn = cjs, \n             gr = NULL, \n             hessian = TRUE, \n             ch, \n             method = \"BFGS\")\nr_time &lt;- (Sys.time() - tic)\nr_ests &lt;- round(plogis(faa$par), digits = 3)\nnames(r_ests) &lt;- c(\"phi\", \"psi\", \"p\")\nprint(r_ests)\n\n  phi   psi     p \n0.625 0.880 0.195 \n\n# TMB - using optim\ntic &lt;- Sys.time()\nobj &lt;- MakeADFun(data = list(ch = ch - 1), # subtract 1 for indexing from 0\n                 parameters = list(b = b), \n                 DLL = \"cjs_tmb\",\n                 silent = TRUE)\nopt &lt;- do.call(\"optim\", obj)\ntmb_time &lt;- (Sys.time() - tic)\ntmb_ests &lt;- round(plogis(opt$par), digits = 3)\nnames(tmb_ests) &lt;- c(\"phi\", \"psi\", \"p\")\nprint(tmb_ests)\n\n  phi   psi     p \n0.625 0.880 0.195 \n\n# clean up\ndyn.unload(dynlib(\"cjs_tmb\"))\n\nWarning: 3 external pointers will be removed\n\nunlink(\"cjs_tmp.*\")\n\n# print times\nprint(r_time)\n\nTime difference of 9.823374 secs\n\nprint(tmb_time)\n\nTime difference of 0.09974098 secs"
  },
  {
    "objectID": "posts/cjs-tmb-is-fast/index.html#conclusion",
    "href": "posts/cjs-tmb-is-fast/index.html#conclusion",
    "title": "CJS is fast in TMB",
    "section": "Conclusion",
    "text": "Conclusion\nIt looks like it works and could be a useful tool for running sensitivity analyses…"
  },
  {
    "objectID": "posts/meet-tweedie/index.html",
    "href": "posts/meet-tweedie/index.html",
    "title": "Meet Tweedie",
    "section": "",
    "text": "I’m working with some spatio-temporally patchy fisheries count data. To allow for their highly variable nature, I was recommended to look at the Tweedie family of distributions. I thought I’d write a post to introduce Tweedie and what I learnt.\n\nTweedie definition\nThe Tweedie distributions are a subfamily of the Exponential distributions, but with a special mean-variance relationship with:\n\na mean of \\(E(Y) = \\mu\\);\na positive dispersion parameter \\(\\sigma^2\\); and\na variance of \\(Var(Y) = \\sigma^2 \\mu^p\\).\n\nThe \\(p\\) in the variance function is often called the “Tweedie power” parameter and acts as an additional shape parameter for the distribution. It is sometimes written in terms of the shape parameter \\(\\alpha\\):\n\\[\np = \\frac{(\\alpha - 2)}{(\\alpha - 1)}\n\\]\nSome familiar distributions are special cases of the Tweedie distribution:\n\n\\(p = 0\\): Normal distribution;\n\\(p = 1\\): Poisson distribution;\n\\(1 &lt; p &lt; 2\\): Compound Poisson/gamma distribution;\n\\(p = 2\\): gamma distribution;\n\\(2 &lt; p &lt; 3\\): Positive stable distributions;\n\\(p = 3\\): Inverse Gaussian distribution / Wald distribution;\n\\(p &gt; 3\\): Positive stable distributions; and\n\\(p = \\infty\\): Extreme stable distributions.\n\n(Note that the distribution is not defined for \\(p\\) values \\(0 &lt; p &lt; 1\\).)\n\n\nPoisson-like Tweedie cases\nSince I am working with counts, the Tweedie distributions related to the Poisson distribution were of particular interest to me. Specifically, I was interested to explore the Tweedie distributions with power parameter values from 1 to 2.\nWith the help of the R package tweedie, we can plot histograms of those distributions as follows:\n\n# libraries\nlibrary(tweedie)\nlibrary(data.table)\nlibrary(ggplot2)\n\n# xis (also known as powers)\nxis &lt;- seq(1, 2, length.out = 12)\n\n# make wide\nd &lt;- as.data.table(lapply(xis, \n                          function(v) \n                            rtweedie(n = 1000, mu = 1, phi = 1, xi = v)))\n\n# set names\nsetnames(d, colnames(d), paste0(\"xi = \", sprintf(\"%.01f\", xis)))\n\n# make long\ndl &lt;- stack(d)\n\n# make plot\np &lt;- ggplot(dl, aes(x = values)) +\n  geom_histogram(binwidth = 0.1) +\n  facet_wrap(~ ind) +\n  labs(x = \"value\", y = \"frequency\") +\n  theme_minimal()\n\n# save plot\njpeg(\"tweedie-histograms.jpg\", res = 300, width = (480 * 5), height = (480 * 5))\nprint(p)\ndev.off()\n\n\n\n\n\n\nIt is interesting to note that as \\(p \\to 1\\) so the Tweedie distribution allows for greater mass of points at 0 (zero). This is a particularly useful feature of the Tweedie when dealing with highly variable and sometimes low counts that are frequently at or around 0.\n\n\nUsing the Tweedie\nI’m going to adapt the example from the tweedie() distribution:\n\n# library\nlibrary(statmod)\n\n# response and explanatory variables\ny &lt;- rgamma(n = 200, scale = 1, shape = 1)\nx &lt;- rpois(200, lambda = 10)\n\n# tweedie profile\nout &lt;- tweedie.profile(y ~ 1, p.vec = seq(1.5, 2.5, by=0.2))\n\nWarning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument\nignored\n\n\n1.5 1.7 1.9 2.1 2.3 2.5 \n......Done.\n\nprint(out$p.max)\n\n[1] 2.05102\n\nprint(out$ci)\n\n[1] 1.885009 2.230231\n\n# fit a poisson generalized linear model with identity link\nm1 &lt;- glm(y ~ x, family = tweedie(var.power = 1, link.power = 1))\nprint(summary(m1))\n\n\nCall:\nglm(formula = y ~ x, family = tweedie(var.power = 1, link.power = 1))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4165  -0.9003  -0.3048   0.4581   2.8102  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.86376    0.25531   3.383 0.000864 ***\nx            0.01391    0.02423   0.574 0.566455    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 0.989408)\n\n    Null deviance: 178.02  on 199  degrees of freedom\nResidual deviance: 177.67  on 198  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n# fit an inverse-Gaussion glm with log-link\nm0 &lt;- glm(y ~ x, family = tweedie(var.power = 3, link.power = 0))\nprint(summary(m0))\n\n\nCall:\nglm(formula = y ~ x, family = tweedie(var.power = 3, link.power = 0))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-12.9320   -1.4851   -0.3246    0.3920    1.8081  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.12647    0.25390  -0.498    0.619\nx            0.01287    0.02408   0.534    0.594\n\n(Dispersion parameter for Tweedie family taken to be 0.9764311)\n\n    Null deviance: 1051.8  on 199  degrees of freedom\nResidual deviance: 1051.5  on 198  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 25\n\n\nI’m going to revisit Tweedie soon, so watch this space…\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{d. gregory2023,\n  author = {D. Gregory, Stephen},\n  title = {Meet {Tweedie}},\n  date = {2023-03-07},\n  url = {https://stephendavidgregory.github.io/posts/meet-tweedie},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nD. Gregory, Stephen. 2023. “Meet Tweedie.” March 7, 2023.\nhttps://stephendavidgregory.github.io/posts/meet-tweedie."
  }
]