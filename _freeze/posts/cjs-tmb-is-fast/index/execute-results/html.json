{
  "hash": "ffe0629537654ab48a7aec2443b3ad55",
  "result": {
    "markdown": "---\ntitle: \"CJS is fast in TMB\"\nsubtitle: \"A simple Cormack Jolly Seber model is fast in Template Model Builder\"\ndate: \"2023-03-10\"\ncategories: [\"statistics\", \"code\"] # environmental-change, tagging-and-telemetry, allee-effects, invasive-species, statistics, code\nimage: \"cjs.jpg\"\nimage-alt: \"How many (over-weight) storks are there?\"\nfilters:\n   - lightbox\nlightbox: auto\nexecute: \n  freeze: true\n---\n\n\nA popular way to estimate life-history rates, such as survival, reproduction and movement rates, is using Cormack Jolly Seber (CJS) models cast in state-space equations. These models vary in complexity, but generally build on two fairly simple equations representing state and observation processes:\n\n$$\nz_{i,t} \\sim \\mbox{Bernoulli}(z_{i,t - 1}, \\varphi)\n$$\n\n$$\ny_{i,t} \\sim \\mbox{Bernoulli}(z_{i,t}, p)\n$$\n\nThe first equation is known as the *state* process and allows for the estimation of the transition rate, here survival $\\varphi$, from the state $z$ of individual $i$ and time $t$ from their state at time $t-1$. The second equation is known as the *observation* process and allows for the estimation of the recapture or resighting rate $p$ of the individual in state $z$ at time $t$.\n\nI've used CJS models in a number of capacities (see ), and for each I've written my own model in [`OpenBUGS`](https://www.mrc-bsu.cam.ac.uk/software/bugs/openbugs/), [`JAGS`](https://mcmc-jags.sourceforge.io/),  [`Stan`](https://mc-stan.org/) or [`nimble`](https://r-nimble.org/), and used Bayesian inference. However, Bayesian inference relies on sampling the parameter space using Monte-Carlo Markov Chains (MCMC), which can be time-consuming (despite ever-efficient algorithms and computation).\n\nRecently, I attended fantastic [`Template Model Builder`](https://kaskr.github.io/adcomp/Introduction.html) training given by [Anders Nielsen](https://orbit.dtu.dk/en/persons/anders-nielsen). It is promoted as a fast and flexible language for inference, whether frequentist or Bayesian.\n\nI thought that it might be nice to write a simple CJS model in TMB, following the excellent examples by [Olivier Gimenez](https://www.cefe.cnrs.fr/fr/recherche/bc/bbp/843-c/253-olivier-gimenez), and in native [`R`](https://www.r-project.org/) too. \n\nHere is my try...\n\n## Data simulation\n\nWe first build a function to simulate data from a multi-event (juvenile and adult) capture-recapture (CJS) model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimul <- function(\n    n_individuals = 1000,\n    n_occasions = 3, # release, first sighting, second sighting\n    n.states = 3, # juvenile, adult, dead\n    phi = 0.7,\n    psi = 0.8,\n    p = 0.3) { \n  \n  # state matrix\n  phi_state <- matrix(c(\n    \n    0, phi, 1 - phi,\n    0, psi, 1 - psi,\n    0, 0, 1\n    \n  ), nrow = n.states, byrow = TRUE)\n  \n  # observation matrix \n  phi_obs <- matrix(c(\n    \n    0, 0, 1,\n    0, p, 1 - p,\n    0, 0, 1\n    \n  ), nrow = n.states, byrow = TRUE)\n  \n  # empty capture history list of matrices\n  ch <- ch_true <- matrix(NA, \n                          nrow = n_individuals, \n                          ncol = n_occasions)\n  \n  # always seen at release\n  ch[, 1] <- ch_true[, 1] <- 1\n  \n  # fill CH\n  for (i in 1:n_individuals) {\n    for (o in 2:n_occasions) {\n      # state transition\n      state_p <- rmultinom(n = 1, \n                           size = 1, \n                           prob = phi_state[ch_true[i, o - 1], ])\n      ch_true[i, o] <- which(state_p == 1)\n      # observation transition\n      event_p <- rmultinom(n = 1, \n                           size = 1, \n                           prob = phi_obs[ch_true[i, o], ])\n      ch[i, o] <- which(event_p == 1)\n    } # o\n  } # i\n  \n  # return\n  return(list(\"ch\" = ch, \"ch.true\" = ch_true))\n  \n}\n```\n:::\n\n\n## Likelihood function in native [`R`](https://www.r-project.org/)\n\nFirst, we need a small function to protect the log from \"exploding\" (thanks, [Olivier Gimenez](https://www.cefe.cnrs.fr/fr/recherche/bc/bbp/843-c/253-olivier-gimenez)):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogprot <- function(v) {\n  eps <- 2.2204e-016\n  u <- log(eps) * (1 + vector(length = length(v)))\n  index <- (v > eps)\n  u[index] <- log(v[index])\n  return(u)\n}\n```\n:::\n\n\nThen the likelihood (take the time to compare this function to the data simulation function - they're similar!):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncjs <- function(b, ch) { \n  \n  # R version of the TMB function below - not used here \n  multvecmat <- function(A, B) {\n    nrowb <- nrow(B)\n    ncolb <- ncol(B)\n    C <- vector(\"numeric\", length = ncolb)\n    for (i in 1:ncolb) {\n      C[i] <- 0\n      for (k in 1:nrowb) {\n        C[i] <- C[i] + A[k] * B[k, i]\n      }\n    }\n    return(C)\n  }\n  \n  # parameters\n  n_individuals <- nrow(ch)\n  n_occasions <- ncol(ch)\n  \n  # transforms\n  phi <- plogis(b[1])\n  psi <- plogis(b[2])\n  p <- plogis(b[3])\n  \n  # state matrix\n  A <- matrix(c(\n    0, phi, 1 - phi,\n    0, psi, 1 - psi,\n    0, 0, 1\n  ), nrow = 3, byrow = TRUE)\n  \n  # observation matrix\n  B <- matrix(c(\n    0, 0, 1,\n    0, p, 1 - p,\n    0, 0, 1\n  ), nrow = 3, byrow = TRUE)\n  \n  # log likelihood\n  l <- 0\n  for (i in 1:n_individuals) {\n    foo <- c(1, 0, 0)\n    for (j in 2:n_occasions) {\n      foo <- (foo %*% A) * B[, ch[i, j]]\n    }\n    l <- l + logprot(sum(foo))\n  }\n  \n  # negative loglikelihood\n  l <- -l\n  \n  # return\n  return(l)\n  \n}\n```\n:::\n\n\n## Likelihood function in [`TMB`](https://kaskr.github.io/adcomp/Introduction.html)\n\nNow, let me try and write the same thing in TMB:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmb_model <- \"\n\n// capture-recapture model\n#include <TMB.hpp>\n\n/* implement the vector - matrix product */\ntemplate<class Type>\nvector<Type> multvecmat(vector<Type> A, matrix<Type> B) {\n  int nrowb = B.rows();\n  int ncolb = B.cols(); \n  vector<Type> C(ncolb);\n  for (int i = 0; i < ncolb; i++) {\n    C(i) = Type(0);\n    for (int k = 0; k < nrowb; k++) {\n      C(i) += A(k) * B(k, i);\n    }\n  }\n  return C;\n}\n\ntemplate<class Type>\nType objective_function<Type>::operator() () {\n\n  // data\n  DATA_IMATRIX(ch);\n  int n_individuals = ch.rows();  \n  int n_occasions = ch.cols();\n  // DATA_VECTOR(fii);\n  \n  // parameters\n  PARAMETER_VECTOR(b);\n  \n  // transformations\n  int npar = b.size();\n  vector<Type> par(npar);\n  for (int i = 0; i < npar; i++) {\n    par(i) = Type(1.0) / (Type(1.0) + exp(-b(i)));\n  }\n  Type phi = par(0);\n  Type psi = par(1);\n  Type p = par(2);\n  \n  // observation matrix\n  matrix<Type> A(3, 3);\n  A(0, 0) = Type(0.0);\n  A(0, 1) = phi;\n  A(0, 2) = Type(1.0) - phi;\n  A(1, 0) = Type(0.0);\n  A(1, 1) = psi;\n  A(1, 2) = Type(1.0) - psi;\n  A(2, 0) = Type(0.0);\n  A(2, 1) = Type(0.0);\n  A(2, 2) = Type(1.0);\n  \n  // observation matrix\n  matrix<Type> B(3, 3);\n  B(0, 0) = Type(0.0);\n  B(0, 1) = Type(0.0);\n  B(0, 2) = Type(1.0);\n  B(1, 0) = Type(0.0);\n  B(1, 1) = p;\n  B(1, 2) = Type(1.0) - p;\n  B(2, 0) = Type(0.0);\n  B(2, 1) = Type(0.0);\n  B(2, 2) = Type(1.0);\n  \n  // likelihood\n  Type ll;\n  Type nll;\n  for (int i = 0; i < n_individuals; i++) {\n    vector<Type> foo(3);\n    foo(0) = Type(1.0);\n    foo(1) = Type(0.0);\n    foo(2) = Type(0.0);\n    for (int j = 1; j < n_occasions; j++) {\n      foo = multvecmat(foo, A) * vector<Type> (B.col(ch(i, j)));\n    }\n    ll += log(sum(foo));\n  }\n  \n  // negative loglikelihood\n  nll = -ll;\n  \n  // return\n  return nll;\n  \n}\"\n```\n:::\n\n\nThen, we write the code to disk, compile and load it, which requires the [`R`](https://www.r-project.org/) package [`TMB`](https://cran.r-project.org/web/packages/TMB/index.html):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite(tmb_model, file = \"cjs_tmb.cpp\")\n\nlibrary(TMB)\nif (!file.exists(\"cjs_tmb.dll\")) compile(\"cjs_tmb.cpp\")\ndyn.load(dynlib(\"cjs_tmb\"))\n```\n:::\n\n\n## Estimation\n\nWith that done, let's see how estimates from the two approaches compare and their speed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data simulation\nraw_data <- simul(\n  n_individuals = 10000,\n  phi = 0.6,\n  psi = 0.9,\n  p = 0.2\n)\n\n# initial parameter values\nb <- runif(3, -1, 1)\n\n# extract the data\nch <- raw_data$ch\n\n# R - using optim\ntic <- Sys.time()\nfaa <- optim(par = b, \n             fn = cjs, \n             gr = NULL, \n             hessian = TRUE, \n             ch, \n             method = \"BFGS\")\nr_time <- (Sys.time() - tic)\nr_ests <- round(plogis(faa$par), digits = 3)\nnames(r_ests) <- c(\"phi\", \"psi\", \"p\")\nprint(r_ests)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  phi   psi     p \n0.625 0.880 0.195 \n```\n:::\n\n```{.r .cell-code}\n# TMB - using optim\ntic <- Sys.time()\nobj <- MakeADFun(data = list(ch = ch - 1), # subtract 1 for indexing from 0\n                 parameters = list(b = b), \n                 DLL = \"cjs_tmb\",\n                 silent = TRUE)\nopt <- do.call(\"optim\", obj)\ntmb_time <- (Sys.time() - tic)\ntmb_ests <- round(plogis(opt$par), digits = 3)\nnames(tmb_ests) <- c(\"phi\", \"psi\", \"p\")\nprint(tmb_ests)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  phi   psi     p \n0.625 0.880 0.195 \n```\n:::\n\n```{.r .cell-code}\n# clean up\ndyn.unload(dynlib(\"cjs_tmb\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: 3 external pointers will be removed\n```\n:::\n\n```{.r .cell-code}\nunlink(\"cjs_tmp.*\")\n\n# print times\nprint(r_time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 9.823374 secs\n```\n:::\n\n```{.r .cell-code}\nprint(tmb_time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 0.09974098 secs\n```\n:::\n:::\n\n\n## Conclusion\n\nIt looks like it works and could be a useful tool for running sensitivity analyses...\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}